<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Monte Carlo vs TD vs Q-learning | ç„¡æ¥µ</title><meta name=keywords content="reinforcement learning"><meta name=description content="Basic Recap Reinforcement learning bases on $V(s),Q(s,a),\pi(a|s),R,G$:
  $V(s)$ : state value, often used in model-based method;
  $Q(s,a)$ : state-action value, often used in model-free method;
 why state-action: $s\rightarrow a$ is defined partly in $\pi(a|s)$, and $V(s,a),\pi(a|s)$ are all parameters inside agent, consequently, $Q(s,a)$ is a combination of $V(s)$ and $\pi(a|s)$.    $\pi(a|s)$ : the policy of a agent, chose a $a$ (action) at a $s$ state;"><meta name=author content="Vac"><link rel=canonical href=https://Vacationwkx.github.io/posts/rl-comparsion/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.bc435bed1061be2618667408894ae8a5d27c970831ce57f287e42f911918fcbc.css integrity="sha256-vENb7RBhviYYZnQIiUropdJ8lwgxzlfyh+QvkRkY/Lw=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://Vacationwkx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://Vacationwkx.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Vacationwkx.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Vacationwkx.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://Vacationwkx.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.82.1"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="Monte Carlo vs TD vs Q-learning"><meta property="og:description" content="Basic Recap Reinforcement learning bases on $V(s),Q(s,a),\pi(a|s),R,G$:
  $V(s)$ : state value, often used in model-based method;
  $Q(s,a)$ : state-action value, often used in model-free method;
 why state-action: $s\rightarrow a$ is defined partly in $\pi(a|s)$, and $V(s,a),\pi(a|s)$ are all parameters inside agent, consequently, $Q(s,a)$ is a combination of $V(s)$ and $\pi(a|s)$.    $\pi(a|s)$ : the policy of a agent, chose a $a$ (action) at a $s$ state;"><meta property="og:type" content="article"><meta property="og:url" content="https://Vacationwkx.github.io/posts/rl-comparsion/"><meta property="og:image" content="https://Vacationwkx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-11-28T21:47:38+08:00"><meta property="article:modified_time" content="2020-11-28T21:47:38+08:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Vacationwkx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Monte Carlo vs TD vs Q-learning"><meta name=twitter:description content="Basic Recap Reinforcement learning bases on $V(s),Q(s,a),\pi(a|s),R,G$:
  $V(s)$ : state value, often used in model-based method;
  $Q(s,a)$ : state-action value, often used in model-free method;
 why state-action: $s\rightarrow a$ is defined partly in $\pi(a|s)$, and $V(s,a),\pi(a|s)$ are all parameters inside agent, consequently, $Q(s,a)$ is a combination of $V(s)$ and $\pi(a|s)$.    $\pi(a|s)$ : the policy of a agent, chose a $a$ (action) at a $s$ state;"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://Vacationwkx.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Monte Carlo vs TD vs Q-learning","item":"https://Vacationwkx.github.io/posts/rl-comparsion/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Monte Carlo vs TD vs Q-learning","name":"Monte Carlo vs TD vs Q-learning","description":"Basic Recap Reinforcement learning bases on $V(s),Q(s,a),\\pi(a|s),R,G$:\n  $V(s)$ : state value, often used in model-based method;\n  $Q(s,a)$ : state-action value, often used in model-free method;\n why state-action: $s\\rightarrow a$ is defined partly in $\\pi(a|s)$, and $V(s,a),\\pi(a|s)$ are all parameters inside agent, consequently, $Q(s,a)$ is a combination of $V(s)$ and $\\pi(a|s)$.    $\\pi(a|s)$ : the policy of a agent, chose a $a$ (action) at a $s$ state;","keywords":["reinforcement learning"],"articleBody":"Basic Recap Reinforcement learning bases on $V(s),Q(s,a),\\pi(a|s),R,G$:\n  $V(s)$ : state value, often used in model-based method;\n  $Q(s,a)$ : state-action value, often used in model-free method;\n why state-action: $s\\rightarrow a$ is defined partly in $\\pi(a|s)$, and $V(s,a),\\pi(a|s)$ are all parameters inside agent, consequently, $Q(s,a)$ is a combination of $V(s)$ and $\\pi(a|s)$.    $\\pi(a|s)$ : the policy of a agent, chose a $a$ (action) at a $s$ state;\n  $R$ : reward, got from each step\n  $G$ : a time-scale reward recording, or a estimate of value for current state. $$ G_t=R_T+\\gamma R_{T-1}+\\gamma^2R_{T-2}+â€¦=\\sum\\limits_{t+1}^{T}\\gamma^{T-i} R $$\n $T$ : Terminal time $\\gamma$ : a self-defined parameter to look how much further into future â€“ long future reward would not affect that much,but instant does. From the equation, the $G$ is influenced by $R$ and $\\gamma$, but for a well-behaved future-telling agent $\\gamma$ is usually set to 1or 0.9, which indicates, for a self-made envornment, $R$ should be set properly to obtain a wanted training result.    A little more from Basic Example : a hero in game, collects he always coins(reward) along a path in a 2d grid map to gain experience\n1\n  Real Existing Items:\nOnce the hero has the real items, it can absolutely get the max reward from environment.\n $G$ represents how many future values the position has, (even $\\gamma$ is also self-defined, but in my view, $\\gamma$ doesnâ€™t affect that much.) and $R$ is what the hero gets from each step in the environment.    Esitimate:\nEstimate is what the hero guess about the $G$, which is $E(G)$. But obviously, in an environment, $G$ is related to state and time, when the hero is exploring with a policy. Then $E(G)$ should be $E_{\\pi}(G_t|S_t=s)$, thatâ€™s what we get from training.\n $v_{\\pi}(s)$ - value function $q_{\\pi}(s,a)$ - action-value function  These 2 are generally the same with $V(s),Q(s,a)$, since basically the policy always exist for most of the agent. The only difference is now they are estimate for $G$ with policy $\\pi$.\n  The FAMOUS Bellman Equation The Bellman equation is basiccally connecting the $v_{\\pi}(s)$ and $v_{\\pi}(s')$, or $q_{\\pi}(s,a)$ and $q_{\\pi}(s',a')$,\n2 $$ \\begin{aligned} v_{\\pi}(s)\u0026=E_{\\pi}[G_t|S_t=s]\\\\\\\n\u0026=E_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_t=s]\\\\\\\n\u0026=?E_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_{t+1}=s']\\\\\\\n\u0026=\\sum_{a\\in A}\\pi(a|s)\\sum_{s'\\in S}p(s',r|s,a) E_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_{t+1}=s']\\\\\\\n\u0026=\\sum_{a\\in A}\\pi(a|s)\\sum_{s'\\in S}p(s',r|s,a) [r+\\gamma E_{\\pi}[G_{t+1}|S_{t+1}=s']\\\\\\\n\u0026=\\sum_{a\\in A}\\pi(a|s)\\sum_{s'\\in S}p(s',r|s,a) [r+\\gamma v_{\\pi}(s')]\\\\\\\n\\end{aligned} $$\nSimilarily explained above, $E(G)$ will become $E_{\\pi}(G_t|S_t=s,A_t=a)$ for $q_{\\pi}(s,a)$, then the Bellman equation changes to:\n$$ \\begin{aligned} q_{\\pi}(s,a)\u0026=E_{\\pi}[G_t|S_t=s,A_t=a]\\\\\\\n\u0026=?E_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_{t+1}=s',A_{t+1}=a']\\\\\\\n\u0026=\\sum_{s'\\in S}p(s',r|s,a)E_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_{t+1}=s']\\\\\\\n\u0026=\\sum_{s'\\in S}p(s',r|s,a)\\sum_{a'\\in A}\\pi(a'|s')E_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_{t+1}=s',A_{t+1}=a']\\\\\\\n\u0026=\\sum_{s'\\in S}p(s',r|s,a)\\sum_{a'\\in A}\\pi(a'|s')[r+\\gamma E_{\\pi}[G_{t+1}|S_{t+1}=s',A_{t+1}=a']]\\\\\\\n\u0026=\\sum_{s'\\in S}p(s',r|s,a)\\sum_{a'\\in A}\\pi(a'|s')[r+\\gamma q_{\\pi}(s',a')]\\\\\\\n\\end{aligned} $$\nChoose Path based on Bellman Equation When the hero stand at $s$ state seeing all $v_{\\pi}(s')$ , but only one step will be chosen in reality, which means $\\pi(a|s)=1$ for this action $a$. This decision will let the $v_{\\pi}(s)$ biggest, and the policy will be updated and $v_*(s)$ is defined as: $$ \\begin{aligned} v_*(s)\u0026=\\max_a \\pi(a|s)\\sum_{s'\\in S}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]\\\\\\\n\u0026=\\sum_{s'\\in S}p(s',r|s,a_{\\max})[r+\\gamma v_{\\pi}(s')]\\\\\\\n\u0026=q_{\\pi}(s,a_{\\max}) \\end{aligned} $$\n$p(s',r|s,a)$ is of course not controlled by the hero, thus, policy has the only option in next step â€“ at $s'$ choose $a'_{\\max}$ , where $q(s',a')$ is max for all $a' \\in A$. Use the same logic,\n$$ \\begin{aligned} q_*(s,a)\u0026=\\sum_{s'\\in S}p(s',r|s,a)[r+\\gamma q(s',a'_{\\max})]\\\\\\\n\\end{aligned} $$\n$v_{\\pi}(s)$ vs $q_{\\pi}(s,a)$ $q_{\\pi}(s,a)$ seems have chosen the $a$ without policy. But thinking deeply, policy $\\pi(a|s)$ controls the choice when finally the hero acts in the environment. The $\\pi$ for $v(s)$ and $q(s,a)$ just dedicates the policy is updated according $v(s)$ and $q(s,a)$.\nNo matter which is used in policy update, what really matters is the next state $s'$, the $v(s')$ or the $\\sum\\limits_{s'\\in S} p(s',r|s,a)[r+\\gamma v(s')] $, since again the $p(s',r|s,a)$ is not controllable.\nOnce the next step is determinated, $a$ at this state $s$ is also confirmed. $q(s,a)$ just more connects to the next state.\n$v(s)$ choses the path by comparsion between multiple $v(s')$, but $q(s,a)$ indicates the path by comparsion between its company $q(s,a_1), q(s,a_2), q(s,a_3)â€¦$.\nUpdate Methods Clarification Monte Carlo, Temporal-Difference and Q-Learning are all model-free methods, which means the probability departing from states to states is unknown. The above optimal policy is used in Dynamic Programming, since the $p(s',r|s,a)$ is known. Thatâ€™s also the reason why use DP in model-based environment. For model-free environment, the value is estimated by exploring and update. MC, TD or Q-learning just differ at these 2 processes.\nMonte Carlo The basic idea of Monte Carlo is to estimate value by : $$ V(s)=\\frac{G}{N} $$\nin the step update form: $$ V(s)\\leftarrow V(s)+\\frac{R-V(s)}{N} $$\nwith starting from $N=1$, in Monte Carlo $V(s)=G$.\nWith this setting, Monte Carlo performs the best with full exploring, also means $\\epsilon=1$ for on policy MC control with $\\epsilon$-soft algorithm, and must run enough steps, which is absolutely slow!!!\nUsing this idea, of course in most environments, exploring start with argmax at policy update will fail.\nNevertheless, the $G$ is driven from trajecotry $\\left[s_0,a_0,r_1,s_1,a_1,r_2,â€¦,s_{T-1},a_{T-1},r_T\\right]$ updated by $G_{s_t}=R_t+\\gamma G_{s_{t-1}}$, where the Terminal $G_{s_T}=0$. No terminal then no value update and policy update. However, a random walk canâ€™t garante reaching the terminal.\n$\\alpha$-constant Monte Carlo The $\\alpha$ - constant Monte Carlo updates it by: $$ \\begin{aligned} V(s)\u0026=V(s)+\\alpha [G-V(s)]\\\\\\\n\u0026=V(s)+\\frac{G-V(s)}{\\frac{1}{\\alpha}}\\\\\\\n\u0026=V(s)(1-\\alpha)+\\alpha G\\\\\\\n\\end{aligned} $$\nIn $\\alpha$ - constant will always consider part of the original $V(s)$: 3\n$$ \\begin{aligned} V_{ep+1}(s)\u0026=[V_{ep-1}(s)(1-\\alpha)+\\alpha G_{ep-1}](1-\\alpha)+\\alpha G_{ep}\\\\\\\n\u0026=V_{ep-1}(1-\\alpha)^2+\\alpha(1-\\alpha)G_{ep-1}+\\alpha G_{ep}\\\\\\\n\u0026=V_1(1-\\alpha)^{ep}+\\sum_1^{ep}\\alpha(1-\\alpha)^iG_i\\\\\\\n\\end{aligned} $$\nfor $\\alpha Whatâ€™s more, when updating the value, the value $V(s)$ is moving towards to the actual value, no matter is updated by Monte Carlo average method or TD or Q-learning, so partly we can trust the new $V(s)$.\nTemporal Difference TD is a bootstrapping method, which is quiet determined by the old value. $$ V_{ep+1}(s)=V_{ep}(s)+\\alpha[R+\\gamma V_{ep}(s')-V_{ep}(s)] $$\nComparing with the $\\alpha$ - constant Monte Carlo $V_{ep+1}(s)=V_{ep}(s)+\\alpha [R_{ep}+\\gamma G_{ep-1}-V_{ep}(s)]$, $\\alpha$ is the stepsize and also determines the update quantity of the $V(s)$. Once $V(s')$ is estimated close to the real value, $V(s)$ is updated by one step closer to the real $V(s)$. Digging to the end, the terminal $V(s_T)=0$, and the $V(s_{T-1})$ s are all updated exactlly by one step close to the real value, unlike the Monte Carlo, always needing a trajectory to end to update the value.\nFor TD, update is not deserved with end to terminal. The first run to terminal is only updated valuable on the $V(s_{T-1})$, and next run is $V(s_{T-2})$, and so onâ€¦\nOn one side, the $V(s)$ is updated truely along the way to terminal, with this chosen path, the value is updated more fast, since the agent prefers to go this path under $\\epsilon$ - greedy policy; On the other side, with randomly exploring, the agent searchs for a better way to terminal. Once found the new path will be compared with the old one, the $V(s)$ will determine the optimal path.\nIf we use $Q(s,a)$ in TD, then the algorithm is called the famous sarsa. $$ Q_{ep+1}(s,a)=Q_{ep}(s,a)+\\alpha[R+\\gamma Q_{ep}(s',a')-Q_{ep}(s,a)] $$ Similarly, the $Q(s,a)$ is updated from the $Q(s_T,a_T)$ once reaches the terminal.\nQ-learning While the agent is still randomly walking in the environment without arriving at the terminal, then the updated value is equavalent to random initialized $Q(s,a)$. The meaningful value is like TD starting from $Q(s_T,a_T)$, the difference locates at that, because of the continous exploring, we can safely choose the best way with fast speed. This indicates we can determine the best step from state $s$ by looking into the $Q(s',a')$s and gives the $s-1$ a symbol ($Q(s,a)$) that $s$ is the best within his company: $$ Q_{ep+1}(s,a)=Q_{ep}(s,a)+\\alpha [R+\\gamma Q_{ep}(s',a'_{\\max})-Q_{ep}(s,a)] $$ Gradually the from the starting state, the agent find the fast by seeing the biggest $Q(s,a)$ at each state.\nOther Thinking Arbitrary Initial Q or V Even give $Q(s,a)$ or $V(s)$ a positive value at start, by updating, a negative value $Q(s',a')$ or $V(s')$ will contribute part of it. At least the $R$ will definately affect negatively to it. After this, a positive $Q(s,a)$ or $V(s)$ canâ€™t be compared with a $Q(s,a)$, which is driven from the positive value given by terminal.\nWhere goes $p(s',r|s,a)$ ? When we have the model, then $p(s',r|s,a)$ can help us compare the $V(s)$ by avioding the low value and passing more though the high value, or directly getting more rewards. In model free, there is no $p(s',r|s,a)$ in offer. But no matter $p(s',r|s,a)$ or $Q(s,a)$ or $V(s)$ just to find the best way. With many exploring, the value is showing the best probability of getting best reward, then there is no need to setting $p(s',r|s,a)$ in model free environment.\n$p(s',r|s,a)$ is of course not controlled by the hero.\n  Online image from here â†©ï¸Ž\n Suttonâ€™s Reinforcement Book â†©ï¸Ž\n The ep represents the episode number, there we use first visit Monte Calro method. â†©ï¸Ž\n   ","wordCount":"1433","inLanguage":"en","datePublished":"2020-11-28T21:47:38+08:00","dateModified":"2020-11-28T21:47:38+08:00","author":{"@type":"Person","name":"Vac"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Vacationwkx.github.io/posts/rl-comparsion/"},"publisher":{"@type":"Organization","name":"ç„¡æ¥µ","logo":{"@type":"ImageObject","url":"https://Vacationwkx.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://Vacationwkx.github.io accesskey=h title="Home (Alt + H)"><img src=/apple-touch-icon.png alt=logo aria-label=logo height=35>Home</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://Vacationwkx.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://Vacationwkx.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://Vacationwkx.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://Vacationwkx.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Vacationwkx.github.io>Home</a>&nbsp;Â»&nbsp;<a href=https://Vacationwkx.github.io/posts/>Posts</a></div><h1 class=post-title>Monte Carlo vs TD vs Q-learning</h1><div class=post-meta>November 28, 2020&nbsp;Â·&nbsp;7 min&nbsp;Â·&nbsp;Vac&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/rl-comparsion.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#basic-recap aria-label="Basic Recap">Basic Recap</a><ul><li><a href=#a-little-more-from-basic aria-label="A little more from Basic">A little more from Basic</a></li><li><a href=#the-famous-bellman-equation aria-label="The FAMOUS Bellman Equation">The FAMOUS Bellman Equation</a></li><li><a href=#choose-path-based-on-bellman-equation aria-label="Choose Path based on Bellman Equation">Choose Path based on Bellman Equation</a></li><li><a href=#v_pis-vs-q_pisa aria-label="$v_{\pi}(s)$ vs $q_{\pi}(s,a)$">$v_{\pi}(s)$ vs $q_{\pi}(s,a)$</a></li></ul></li><li><a href=#update-methods-clarification aria-label="Update Methods Clarification">Update Methods Clarification</a><ul><li><a href=#monte-carlo aria-label="Monte Carlo">Monte Carlo</a></li><li><a href=#alpha-constant-monte-carlo aria-label="$\alpha$-constant Monte Carlo">$\alpha$-constant Monte Carlo</a></li><li><a href=#temporal-difference aria-label="Temporal Difference">Temporal Difference</a></li><li><a href=#q-learning aria-label=Q-learning>Q-learning</a></li></ul></li><li><a href=#other-thinking aria-label="Other Thinking">Other Thinking</a><ul><li><a href=#arbitrary-initial-q-or-v aria-label="Arbitrary Initial Q or V">Arbitrary Initial Q or V</a></li><li><a href=#where-goes-psrsa- aria-label="Where goes $p(s',r|s,a)$ ?">Where goes $p(s',r|s,a)$ ?</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=basic-recap>Basic Recap<a hidden class=anchor aria-hidden=true href=#basic-recap>#</a></h1><p>Reinforcement learning bases on $V(s),Q(s,a),\pi(a|s),R,G$:</p><ul><li><p>$V(s)$ : state value, often used in model-based method;</p></li><li><p>$Q(s,a)$ : state-action value, often used in model-free method;</p><ul><li>why state-action: $s\rightarrow a$ is defined partly in $\pi(a|s)$, and $V(s,a),\pi(a|s)$ are all parameters inside agent, consequently, $Q(s,a)$ is a combination of $V(s)$ and $\pi(a|s)$.</li></ul></li><li><p>$\pi(a|s)$ : the policy of a agent, chose a $a$ (action) at a $s$ state;</p></li><li><p>$R$ : reward, got from each step</p></li><li><p>$G$ : a time-scale reward recording, or a estimate of value for current state.
$$
G_t=R_T+\gamma R_{T-1}+\gamma^2R_{T-2}+&mldr;=\sum\limits_{t+1}^{T}\gamma^{T-i} R
$$</p><ul><li>$T$ : Terminal time</li><li>$\gamma$ : a self-defined parameter to look how much further into future &ndash; long future reward would not affect that much,but instant does.</li><li>From the equation, the $G$ is influenced by $R$ and $\gamma$, but for a well-behaved future-telling agent $\gamma$ is usually set to 1or 0.9, which indicates, for a self-made envornment, $R$ should be set properly to obtain a wanted training result.</li></ul></li></ul><h2 id=a-little-more-from-basic>A little more from Basic<a hidden class=anchor aria-hidden=true href=#a-little-more-from-basic>#</a></h2><p><strong>Example</strong> : a hero in game, collects he always coins(reward) along a path in a 2d grid map to gain experience</p><p><img loading=lazy src=https://miro.medium.com/freeze/max/588/1*Lq_shZnfjjiFEBmBOHk_qA.gif alt="a hero in a 2D map">
<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><ul><li><p>Real Existing Items:</p><p>Once the hero has the real items, it can absolutely get the max reward from environment.</p><ul><li>$G$ represents how many future values the position has, (even $\gamma$ is also self-defined, but in my view, $\gamma$ doesn&rsquo;t affect that much.)</li><li>and $R$ is what the hero gets from each step in the environment.</li></ul></li><li><p>Esitimate:</p><p>Estimate is what the hero guess about the $G$, which is $E(G)$. But obviously, in an environment, $G$ is related to state and time, when the hero is exploring with a policy. Then $E(G)$ should be $E_{\pi}(G_t|S_t=s)$, that&rsquo;s what we get from training.</p><ul><li>$v_{\pi}(s)$ - value function</li><li>$q_{\pi}(s,a)$ - action-value function</li></ul><p>These 2 are generally the same with $V(s),Q(s,a)$, since basically the policy always exist for most of the agent. The only difference is now they are estimate for $G$ with policy $\pi$.</p></li></ul><h2 id=the-famous-bellman-equation>The FAMOUS Bellman Equation<a hidden class=anchor aria-hidden=true href=#the-famous-bellman-equation>#</a></h2><p>The Bellman equation is basiccally connecting the $v_{\pi}(s)$ and $v_{\pi}(s')$, or $q_{\pi}(s,a)$ and $q_{\pi}(s',a')$,</p><p><img loading=lazy src=/rl/vs.png alt="Backup Diagramm">
<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>
$$
\begin{aligned}
v_{\pi}(s)&=E_{\pi}[G_t|S_t=s]\\\<br>&=E_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t=s]\\\<br>&=?E_{\pi}[R_{t+1}+\gamma G_{t+1}|S_{t+1}=s']\\\<br>&=\sum_{a\in A}\pi(a|s)\sum_{s'\in S}p(s',r|s,a) E_{\pi}[R_{t+1}+\gamma G_{t+1}|S_{t+1}=s']\\\<br>&=\sum_{a\in A}\pi(a|s)\sum_{s'\in S}p(s',r|s,a) [r+\gamma E_{\pi}[G_{t+1}|S_{t+1}=s']\\\<br>&=\sum_{a\in A}\pi(a|s)\sum_{s'\in S}p(s',r|s,a) [r+\gamma v_{\pi}(s')]\\\<br>\end{aligned}
$$</p><p>Similarily explained above, $E(G)$ will become $E_{\pi}(G_t|S_t=s,A_t=a)$ for $q_{\pi}(s,a)$, then the Bellman equation changes to:</p><p>$$
\begin{aligned}
q_{\pi}(s,a)&=E_{\pi}[G_t|S_t=s,A_t=a]\\\<br>&=?E_{\pi}[R_{t+1}+\gamma G_{t+1}|S_{t+1}=s',A_{t+1}=a']\\\<br>&=\sum_{s'\in S}p(s',r|s,a)E_{\pi}[R_{t+1}+\gamma G_{t+1}|S_{t+1}=s']\\\<br>&=\sum_{s'\in S}p(s',r|s,a)\sum_{a'\in A}\pi(a'|s')E_{\pi}[R_{t+1}+\gamma G_{t+1}|S_{t+1}=s',A_{t+1}=a']\\\<br>&=\sum_{s'\in S}p(s',r|s,a)\sum_{a'\in A}\pi(a'|s')[r+\gamma E_{\pi}[G_{t+1}|S_{t+1}=s',A_{t+1}=a']]\\\<br>&=\sum_{s'\in S}p(s',r|s,a)\sum_{a'\in A}\pi(a'|s')[r+\gamma q_{\pi}(s',a')]\\\<br>\end{aligned}
$$</p><h2 id=choose-path-based-on-bellman-equation>Choose Path based on Bellman Equation<a hidden class=anchor aria-hidden=true href=#choose-path-based-on-bellman-equation>#</a></h2><p>When the hero stand at $s$ state seeing all $v_{\pi}(s')$ , but only one step will be chosen in reality, which means $\pi(a|s)=1$ for this action $a$. This decision will let the $v_{\pi}(s)$ biggest, and the policy will be updated and $v_*(s)$ is defined as:
$$
\begin{aligned}
v_*(s)&=\max_a \pi(a|s)\sum_{s'\in S}p(s',r|s,a)[r+\gamma v_{\pi}(s')]\\\<br>&=\sum_{s'\in S}p(s',r|s,a_{\max})[r+\gamma v_{\pi}(s')]\\\<br>&=q_{\pi}(s,a_{\max})
\end{aligned}
$$</p><p>$p(s',r|s,a)$ is of course not controlled by the hero, thus, policy has the only option in next step &ndash; at $s'$ choose $a'_{\max}$ , where $q(s',a')$ is max for all $a' \in A$. Use the same logic,</p><p>$$
\begin{aligned}
q_*(s,a)&=\sum_{s'\in S}p(s',r|s,a)[r+\gamma q(s',a'_{\max})]\\\<br>\end{aligned}
$$</p><h2 id=v_pis-vs-q_pisa>$v_{\pi}(s)$ vs $q_{\pi}(s,a)$<a hidden class=anchor aria-hidden=true href=#v_pis-vs-q_pisa>#</a></h2><p>$q_{\pi}(s,a)$ seems have chosen the $a$ without policy. But thinking deeply, policy $\pi(a|s)$ controls the choice when finally the hero acts in the environment. The $\pi$ for $v(s)$ and $q(s,a)$ just dedicates the policy is updated according $v(s)$ and $q(s,a)$.</p><p>No matter which is used in policy update, what really matters is the next state $s'$, the $v(s')$ or the $\sum\limits_{s'\in S} p(s',r|s,a)[r+\gamma v(s')] $, since again the $p(s',r|s,a)$ is not controllable.</p><p>Once the next step is determinated, $a$ at this state $s$ is also confirmed. $q(s,a)$ just more connects to the next state.</p><p>$v(s)$ choses the path by comparsion between multiple $v(s')$, but $q(s,a)$ indicates the path by comparsion between its company $q(s,a_1), q(s,a_2), q(s,a_3)&mldr;$.</p><h1 id=update-methods-clarification>Update Methods Clarification<a hidden class=anchor aria-hidden=true href=#update-methods-clarification>#</a></h1><p>Monte Carlo, Temporal-Difference and Q-Learning are all model-free methods, which means the probability departing from states to states is unknown. The above optimal policy is used in Dynamic Programming, since the $p(s',r|s,a)$ is known. That&rsquo;s also the reason why use DP in model-based environment. For model-free environment, the value is estimated by exploring and update. MC, TD or Q-learning just differ at these 2 processes.</p><h2 id=monte-carlo>Monte Carlo<a hidden class=anchor aria-hidden=true href=#monte-carlo>#</a></h2><p>The basic idea of Monte Carlo is to estimate value by :
$$
V(s)=\frac{G}{N}
$$</p><p>in the step update form:
$$
V(s)\leftarrow V(s)+\frac{R-V(s)}{N}
$$</p><p>with starting from $N=1$, in Monte Carlo $V(s)=G$.</p><p>With this setting, Monte Carlo performs the best with full exploring, also means $\epsilon=1$ for on policy MC control with $\epsilon$-soft algorithm, and must run enough steps, which is absolutely slow!!!</p><p>Using this idea, of course in most environments, exploring start with argmax at policy update will fail.</p><p>Nevertheless, the $G$ is driven from trajecotry $\left[s_0,a_0,r_1,s_1,a_1,r_2,&mldr;,s_{T-1},a_{T-1},r_T\right]$ updated by $G_{s_t}=R_t+\gamma G_{s_{t-1}}$, where the Terminal $G_{s_T}=0$. No terminal then no value update and policy update. However, a random walk can&rsquo;t garante reaching the terminal.</p><h2 id=alpha-constant-monte-carlo>$\alpha$-constant Monte Carlo<a hidden class=anchor aria-hidden=true href=#alpha-constant-monte-carlo>#</a></h2><p>The $\alpha$ - constant Monte Carlo updates it by:
$$
\begin{aligned}
V(s)&=V(s)+\alpha [G-V(s)]\\\<br>&=V(s)+\frac{G-V(s)}{\frac{1}{\alpha}}\\\<br>&=V(s)(1-\alpha)+\alpha G\\\<br>\end{aligned}
$$</p><p>In $\alpha$ - constant will always consider part of the original $V(s)$: <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><p>$$
\begin{aligned}
V_{ep+1}(s)&=[V_{ep-1}(s)(1-\alpha)+\alpha G_{ep-1}](1-\alpha)+\alpha G_{ep}\\\<br>&=V_{ep-1}(1-\alpha)^2+\alpha(1-\alpha)G_{ep-1}+\alpha G_{ep}\\\<br>&=V_1(1-\alpha)^{ep}+\sum_1^{ep}\alpha(1-\alpha)^iG_i\\\<br>\end{aligned}
$$</p><p>for $\alpha &lt;1$, when $t\rightarrow \infty$, $V_{\infty}$ has more value depending on $G$, and specially recent $G$.</p><p>What&rsquo;s more, when updating the value, the value $V(s)$ is moving towards to the actual value, no matter is updated by Monte Carlo average method or TD or Q-learning, so partly we can trust the new $V(s)$.</p><h2 id=temporal-difference>Temporal Difference<a hidden class=anchor aria-hidden=true href=#temporal-difference>#</a></h2><p>TD is a bootstrapping method, which is quiet determined by the old value.
$$
V_{ep+1}(s)=V_{ep}(s)+\alpha[R+\gamma V_{ep}(s')-V_{ep}(s)]
$$</p><p>Comparing with the $\alpha$ - constant Monte Carlo $V_{ep+1}(s)=V_{ep}(s)+\alpha [R_{ep}+\gamma G_{ep-1}-V_{ep}(s)]$, $\alpha$ is the stepsize and also determines the update quantity of the $V(s)$. Once $V(s')$ is estimated close to the real value, $V(s)$ is updated by one step closer to the real $V(s)$. Digging to the end, the terminal $V(s_T)=0$, and the $V(s_{T-1})$ s are all updated exactlly by one step close to the real value, unlike the Monte Carlo, always needing a trajectory to end to update the value.</p><p>For TD, update is not deserved with end to terminal. The first run to terminal is only updated valuable on the $V(s_{T-1})$, and next run is $V(s_{T-2})$, and so on&mldr;</p><p>On one side, the $V(s)$ is updated truely along the way to terminal, with this chosen path, the value is updated more fast, since the agent prefers to go this path under $\epsilon$ - greedy policy; On the other side, with randomly exploring, the agent searchs for a better way to terminal. Once found the new path will be compared with the old one, the $V(s)$ will determine the optimal path.</p><p>If we use $Q(s,a)$ in TD, then the algorithm is called the famous <strong>sarsa</strong>.
$$
Q_{ep+1}(s,a)=Q_{ep}(s,a)+\alpha[R+\gamma Q_{ep}(s',a')-Q_{ep}(s,a)]
$$
Similarly, the $Q(s,a)$ is updated from the $Q(s_T,a_T)$ once reaches the terminal.</p><h2 id=q-learning>Q-learning<a hidden class=anchor aria-hidden=true href=#q-learning>#</a></h2><p>While the agent is still randomly walking in the environment without arriving at the terminal, then the updated value is equavalent to random initialized $Q(s,a)$. The meaningful value is like TD starting from $Q(s_T,a_T)$, the difference locates at that, because of the continous exploring, we can safely choose the best way with fast speed. This indicates we can determine the best step from state $s$ by looking into the $Q(s',a')$s and gives the $s-1$ a symbol ($Q(s,a)$) that $s$ is the best within his company:
$$
Q_{ep+1}(s,a)=Q_{ep}(s,a)+\alpha [R+\gamma Q_{ep}(s',a'_{\max})-Q_{ep}(s,a)]
$$
Gradually the from the starting state, the agent find the fast by seeing the biggest $Q(s,a)$ at each state.</p><h1 id=other-thinking>Other Thinking<a hidden class=anchor aria-hidden=true href=#other-thinking>#</a></h1><h2 id=arbitrary-initial-q-or-v>Arbitrary Initial Q or V<a hidden class=anchor aria-hidden=true href=#arbitrary-initial-q-or-v>#</a></h2><p>Even give $Q(s,a)$ or $V(s)$ a positive value at start, by updating, a negative value $Q(s',a')$ or $V(s')$ will contribute part of it. At least the $R$ will definately affect negatively to it. After this, a positive $Q(s,a)$ or $V(s)$ can&rsquo;t be compared with a $Q(s,a)$, which is driven from the positive value given by terminal.</p><h2 id=where-goes-psrsa->Where goes $p(s',r|s,a)$ ?<a hidden class=anchor aria-hidden=true href=#where-goes-psrsa->#</a></h2><p>When we have the model, then $p(s',r|s,a)$ can help us compare the $V(s)$ by avioding the low value and passing more though the high value, or directly getting more rewards. In model free, there is no $p(s',r|s,a)$ in offer. But no matter $p(s',r|s,a)$ or $Q(s,a)$ or $V(s)$ just to find the best way. With many exploring, the value is showing the best probability of getting best reward, then there is no need to setting $p(s',r|s,a)$ in model free environment.</p><p>$p(s',r|s,a)$ is of course not controlled by the hero.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Online image from <a href=https://miro.medium.com/freeze/max/588/1*Lq_shZnfjjiFEBmBOHk_qA.gif>here</a> <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Sutton&rsquo;s Reinforcement Book <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>The ep represents the episode number, there we use first visit Monte Calro method. <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><footer class=post-footer><ul class=post-tags><li><a href=https://Vacationwkx.github.io/tags/reinforcement-learning/>reinforcement learning</a></li></ul><nav class=paginav><a class=prev href=https://Vacationwkx.github.io/posts/rl-update-equation/><span class=title>Â« Prev Page</span><br><span>Value Update Comparsion among Basical RL</span></a>
<a class=next href=https://Vacationwkx.github.io/posts/new-on-hugo/><span class=title>Next Page Â»</span><br><span>New on HugoðŸ¤º</span></a></nav></footer><div id=disqus_thread></div><script>(function(){var a=document,b=a.createElement('script');b.src='https://github-on-hugo.disqus.com/embed.js',b.setAttribute('data-timestamp',+new Date),(a.head||a.body).appendChild(b)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></article></main><footer class=footer><span>&copy; 2021 <a href=https://Vacationwkx.github.io>ç„¡æ¥µ</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button>
</a><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script><script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script></body></html>