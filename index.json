[{"content":"Rusty Lake Rusty lake is a thrilling game series with many secrets in them. Animal characters, shadow person, religious elements, murders\u0026hellip; A thriller is too less to cover the topic of rusty lake, and what I am intent to do, is to reveal the prossible secrest hidden behind the screen\u0026hellip;\nCubes Collection The Lake The lake\u0026rsquo;s goal is to find the 5 objects:\n apple coin gem shell eye  and attach them all on the tree at corresponding position. Some interesting thing happened in-between:\n the tree comes out from a fish using mold as bait, we fished a dead body magnet attracts a chest requiring code from The Season  We come to a cabin. All the things connect to fishing.\nWorm-\u0026gt;shrimp-\u0026gt; key \u0026amp; shrimp head-\u0026gt;worm2 \u0026amp; magnet\nWorm2-\u0026gt;fish-\u0026gt;tree\nShrimp head-\u0026gt;dead body\u0026ndash;knife\u0026ndash;\u0026gt;apple\nMagnet-\u0026gt;key2 \u0026amp; 2 chests -\u0026gt;coin\nChest1-\u0026gt; gem\nChest2-\u0026gt; \u0026gt; green gem\ncrowbar-\u0026gt;wall-\u0026gt;key-\u0026gt;mold-\u0026gt;fishing-\u0026gt;shell\ncrowbar-\u0026gt;wall2-\u0026gt;eye\nblue gem-\u0026gt; pentagram-\u0026gt;shadow person( fail of the mental health healing)\n the 5 wounds of Christ1 upside down Santa1 Union of Opposite2 after Renassiance-era, pentagram is considered as an apotropaic charm to protect against evil forces3  green gem-\u0026gt; pentagon-\u0026gt; black cubes\n related to Saturn Worship4 in acient Egypt, Isis is the eldest daughter of Saturn5  The Season Spring: 1964 What do i remember?\nSamara\u0026rsquo;photo\nout of window is samara\nboard:\n Prozac (百忧解): antidepressant, but why? Harvey\u0026rsquo;s Egg -\u0026gt; cubes Mental Health \u0026amp; Finisishing: The Lake???  bird food-\u0026gt; bird egg-\u0026gt; cook-\u0026gt;black butterfly\nSymbolic Meaning of a Black Butterfly\n  sometimes positive, and sometimes omnious\n  transition, renewal or rebirth\n  phone:\n all that you touched, you change  2nd time\nseed\nfree the bird\nSummer: 1971 moon: samara is killed by the shadow person, code: 1487\nlake: woman in water-\u0026gt;shadow person\nphone:\n there will be blood  board:\n Remember that song? prozac mental health\u0026hellip;..  2nd time\nwater around the oven\nFall: 1971 all blood\nkill the bird\neye on the dash\nboard:\n im afraid i will do something horrible  phone:\n The past is never dead, it\u0026rsquo;s not even past  photo:\n dead: 12-10-1997 pink blossom  2nd time\nmushroom next to the oven\nWinter: 1981 photo:\n samara: 09:05 lake: 03:55 dead: 11:25 blue cube: 13:10  board:\n change the past phone+fuel=change past cube+fire=escape memory blue-cube+fire=memory  Clock:\n it\u0026rsquo;s me  2nd time\n flower from bird fruit mushroom prozac  go back to the spring\nAlres oct 1888\nwhat do i remember?\n best wine: Absinthe code: 1458  Havey\u0026rsquo;s Box  egg fish spoon green glass  card: moving to rusty lake: mental health \u0026amp; fishing\ncumb: song\nBox:\n what is this place? I need to get out of here  3 fireflies : finally: shadow person with wings\nCase 23 1 - Murder Scene Fall 1971\n4 words-\u0026gt;fate\ncall mental health \u0026amp; fishing : 024355\n i am sorry your place is already taken  Black butterfly from the mouth of the lady -\u0026gt; 1421-\u0026gt;evidence\n2 - The Inversitigation connect to mill,\ndetective put the 2 cube in tv\nmap:\n capel mill cabin cave brighe  3 - The Capel map in the 2, the misters place, see the lake and cabin\n4 - The Cabin excape from the dear killer\nMill can see the capel, owl saved the detective\u0026rsquo;s life\nwhat happen in the lake place\ngirl and detective,\nold man and wife, mr crow, mr owl\n   https://www.learnreligions.com/pentagrams-4123031 \u0026#x21a9;\u0026#xfe0e;\n https://www.newworldencyclopedia.org/entry/Pentagram \u0026#x21a9;\u0026#xfe0e;\n https://en.wikipedia.org/wiki/Pentagram \u0026#x21a9;\u0026#xfe0e;\n https://phoenixesotericsociety.com/black-cube-symbolism/ \u0026#x21a9;\u0026#xfe0e;\n http://hollywoodsubliminals.wordpress.com/black-cube/ \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://Vacationwkx.github.io/posts/rustylake/","summary":"Rusty Lake Rusty lake is a thrilling game series with many secrets in them. Animal characters, shadow person, religious elements, murders\u0026hellip; A thriller is too less to cover the topic of rusty lake, and what I am intent to do, is to reveal the prossible secrest hidden behind the screen\u0026hellip;\nCubes Collection The Lake The lake\u0026rsquo;s goal is to find the 5 objects:\n apple coin gem shell eye  and attach them all on the tree at corresponding position.","title":"Rusty lake - Game Timeline \u0026 Secrets"},{"content":"Reinforcement Learninng  check probability is correct replay reuse, not always interacting, split into stages, or types  Computer Vision  build 3d for object classification position for area + class label power up classification for multiple objects in one scene parameter tuning under reinforcement learning: link accuracy as reward, parameter as actions  Natural Language Processing  from language to video building voice control to programming  voice control to logica, puls add or minus more complicated logic    ","permalink":"https://Vacationwkx.github.io/posts/ai/","summary":"Reinforcement Learninng  check probability is correct replay reuse, not always interacting, split into stages, or types  Computer Vision  build 3d for object classification position for area + class label power up classification for multiple objects in one scene parameter tuning under reinforcement learning: link accuracy as reward, parameter as actions  Natural Language Processing  from language to video building voice control to programming  voice control to logica, puls add or minus more complicated logic    ","title":"AI"},{"content":"State Representation  state vector ( obey Markov Property) observation →knowledge→ state Partially observable Markov decision process \u0026ldquo;learning or classification algorithms to \u0026ldquo;learn\u0026rdquo; those states\u0026rdquo;  A simple linear regression A more complex non-linear function approximator, such as a multi-layer neural network.    The Atari DQN work by DeepMind team used a combination of feature engineering and relying on deep neural network to achieve its results. The feature engineering included downsampling the image, reducing it to grey-scale and - importantly for the Markov Property - using four consecutive frames to represent a single state, so that information about velocity of objects was present in the state representation. The DNN then processed the images into higher-level features that could be used to make predictions about state values.\nPolicy Gradients Deep Deterministic Policy Gradient - Spinning Up documentation\nMathe Funda\nDeep Reinforcement Learning VDQN DeepMind used atari environment for DQN test, even through all the return observation for preprocessing:\nPreprocessing  Observation  rgb $\\rightarrow$ gray, i.e. image shape (210,160,3)$\\rightarrow$ (210,160) down sample: (210,160) $\\rightarrow$ (110,84) crop: (110,84) $\\rightarrow$ (84,84)   Observation :arrow_right: state:  4 history frames to 1 $\\rightarrow$ (84,84,4)    CNN   architecture\n Conv2D: 32, 8x8, strides=4, input_shape=(84,84,4) Conv2D: 64, 4x4, strides=2 Conv2D: 64, 3x3, strides=1 Dense: 256 Dens: outputshape    comple\n RMSProp    Replay Buffer  fix length every time feed: (state, action, reward, next_state, done) once reach L length: start training length: 1million  Target model update  every C step Epsilon: 1$\\rightarrow$0.1 (1 million frame for total 50 milion)  Frame skipping great explanation\n chose at kth frame last k-1 frame k=4  Speed up for atari, use info['ale.lives'] \u0026lt; 5 for terminating the episode\nClip  reward: -1,0,1 Error: $|Q(s,a,\\theta)-Q(s',a',\\theta^-)\\le1$  NOTES:\nchange RMSprop parameter\ntf.keras.optimizers.RMSprop( learning_rate=0.00025, rho=0.9, momentum=0.95, epsilon=1e-07, centered=False, name=\u0026#34;RMSprop\u0026#34;, **kwargs ) Random Gym Run  Frame:10,000 Reward: 4-2, 5-0  Double DQN  main: choose action target: update Q in Bellman as Q_max model fit: fit the main with target value output after same iterations, copy main weights and biases to target  DuelingDQN maybe drop DQN\nsource\n DDQN Architecture custom model fit for weights too complicated Priorited Replay  paper code test   Agent Train Main  DQN Parameter Adjustment https://github.com/dennybritz/reinforcement-learning/issues/30\nhttps://www.reddit.com/r/reinforcementlearning/comments/7kwcb5/need_help_how_to_debug_deep_rl_algorithms/\nA3C  David Sliver Policy Gradient Mathe Paper review Example threading Pytorch Code for Pendulum  A2C Why A2C not A3C\n Two head Network Why Entropy: https://awjuliani.medium.com/maximum-entropy-policies-in-reinforcement-learning-everyday-life-f5a1cc18d32d#:~:text=Because%20RL%20is%20all%20about,the%20actions%20an%20agent%20takes.\u0026amp;text=In%20RL%2C%20the%20goal%20is,term%20sum%20of%20discounted%20rewards  Entropy is great, but you might be wondering what that has to do with reinforcement learning and this A2C algorithm we discussed. The idea here is to use entropy to encourage further exploration of the model to prevent premature convergence   Negative Loss:  TensorFlow and PyTorch currently don’t have the ability to maximize a function, we then minimize the negative of our loss   Accumulated gradient  PPO   debug for sub optimal:\n  possible solutions\n decrease lr decrease lambda during program    It would be helpful to output more metrics, such as losses, norms of the gradients, KL divergence between your old and new policies after a number of PPO updates.source\n  change algo\n It depends on the environment. Something like ball balancing might just tend to destabilize with PPO, vs. something like half cheetah that is less finicky balance wise. You might try using td3 or sac, but with ppo you might just have to early stop. There might be some perfect combo of lr and clip param that leaves it stabilized\u0026hellip; maybe with using another optimizer as well like classic momentum or adagrad.\nsource\n      Material Powerup Knowledge  How to Exploration Why RL Hard DRL Sucks  Intro An Introduction to Deep Reinforcement Learning\nDRL Reward shaping reference\nCode for model  Tensorflow Pytorch  Simple to create Model PPO    Course   CS 285\n  Deep Reinforcement Learning\n  Blog  Atari Medium Tensorflow Code Start from PONG Nice Dude  Atari  Monitor / Video using X11 Recap  CNN  parameter calculation : https://medium.com/@iamvarman/how-to-calculate-the-number-of-parameters-in-the-cnn-5bd55364d7ca#:~:text=To%20calculate%20it%2C%20we%20have,3%E2%80%931))%20%3D%2048 output shape calculation  API for Vrep Legacy remote API\nRemote API functions (Python)\nDavid Silver - 4/10 Teaching - David Silver\nThe Book - Ch5 Reinforcement Learning: An Introduction\nCode for it:\nReinforcement Learning: An Introduction\nPaper Keypaper\nTraining Software Gym: A toolkit for developing and comparing reinforcement learning algorithms\nOpen cv  resize  ","permalink":"https://Vacationwkx.github.io/posts/rl-notes/","summary":"State Representation  state vector ( obey Markov Property) observation →knowledge→ state Partially observable Markov decision process \u0026ldquo;learning or classification algorithms to \u0026ldquo;learn\u0026rdquo; those states\u0026rdquo;  A simple linear regression A more complex non-linear function approximator, such as a multi-layer neural network.    The Atari DQN work by DeepMind team used a combination of feature engineering and relying on deep neural network to achieve its results. The feature engineering included downsampling the image, reducing it to grey-scale and - importantly for the Markov Property - using four consecutive frames to represent a single state, so that information about velocity of objects was present in the state representation.","title":"Reinforcement Learning Notes"},{"content":"In any case, you will need to get started with reading up on machine learning. i would suggest you first briefly start with neural networks, followed by deep convolutional neural networks just to get used to the idea of deep learning and subsequently focus on picking up reinforcement learning. As this machine learning field heavily requires you to be able to do programming, I would suggest that you try to implement the models you learnt and get familiar with the various packages of pytorch, tensorflow, fastai.\nCourse CV  CS231n Winter 2016 Course | DEV290x | edX  ML  Coursera  DL Fundamental Recap Fastai   use Colab: fast set up, free | ahah, too slow, 👋🏻\n  use GCP again\n  gcloud compute config -ssh\n  get error like:\n# error type 1 External IP address was not found; defaulting to using IAP tunneling. ERROR: (gcloud.compute.start-iap-tunnel) Error while connecting [4033: u\u0026#39;not authorized\u0026#39;]. kex_exchange_identification: Connection closed by remote host ERROR: (gcloud.compute.ssh) [/usr/bin/ssh] exited with return code [255]. # error type 2 ssh: connect to host 34.**.**.167 port 22: Resource temporarily unavailable ERROR: (gcloud.compute.ssh) [/usr/bin/ssh] exited with return code [255]. try this :(quelle)\n    use WSL in windows\n  3 tips\n  can\u0026rsquo;t get app-key, find solution\nsudo apt remove gpg sudo apt-get update -y sudo apt-get install -y gnupg1   fresh hand following the tutorial, get cricked out because of ssh update after the buildung the instance. find solution here. MAKE SURE THE INSTANCE IS RUNNING!!!\n    Michael Nielsen \u0026rsquo;s guide Neural networks and deep learning\n  Tasks\n  Neural Networks and Deep Learning\n  What this book is about\n  On the exercises and problems\n  Using neural nets to recognize handwritten digits\n  How the backpropagation algorithm works / 3\n  Improving the way neural networks learn - 3/6\n  A visual proof that neural nets can compute any function - 0/6\n  Why are deep neural networks hard to train? - 0/4\n  give up here since of the old code\n  Deep learning - 0/6\n  Appendix: Is there a simple algorithm for intelligence?\n  Acknowledgements\n    Frequently Asked Questions\n  Hyper Parameter Choosing Hyperopt Documentation\nRandom search for hyper-parameter optimization\nPractical recommendations for gradient-based training of deep architectures\nCNN  A great intro Tips from stanford  Tip 1 Tip 2 Tip 3   Loss Function  keras choose   GIF for better understanding CNN Architecture LeNet - 5  Softmax or Sigmoid maybe answer\nLoss Analysis Validation Loss\nNvidia-smi\n","permalink":"https://Vacationwkx.github.io/posts/nn-notes/","summary":"In any case, you will need to get started with reading up on machine learning. i would suggest you first briefly start with neural networks, followed by deep convolutional neural networks just to get used to the idea of deep learning and subsequently focus on picking up reinforcement learning. As this machine learning field heavily requires you to be able to do programming, I would suggest that you try to implement the models you learnt and get familiar with the various packages of pytorch, tensorflow, fastai.","title":"NNN notes"},{"content":"This blog shows up a way to ssh a ubuntu computer (as Jupyter Notebook server) from mac.\nPrerequirements Ubuntu / Remote Setting SSH Setting run the following command to enable the ssh on ubuntu(18.04)\n$ sudo apt-get install openssh-server Jupyter Notebook Setting(optional) using jupyter notebook may require the token or password, you may refer to the jupyter notebook official site for details.\n  Token\ntake note of the resultjupyter notebook command, the token is inside the link after token=, e.g\n$ http://localhost:8888/?token=c8de56fa4deed24899803e93c227592aef6538f93025fe01   Password\nto set the password, first run\n$ jupyter notebook --generate-config to generate jupyter_notebook_config.py, see where it locate at official site.\nthen run\n$ jupyter notebook password Enter password: **** Verify password: **** to set the password, afterwards all localhost:XXXX can log in with this password\n  Run Jupyter at Mac only 2 commands:\n Ubuntu: jupyter notebook --no-browser --port=XXXX Mac: ssh -N -f -L localhost:YYYY:localhost:XXXX remoteuser@remotehost or ssh -L localhost:YYYY:localhost:XXXX remoteuser@remotehost for not running in background, in case the port usage conflict  you may set up a script refer to Lj Miranda\u0026rsquo;s post,\nand remove the forward port rule refer to this question on StackExchange, or simply reboot your mac\nIf remote is HPC (NUS) Terminal 1 (after ssh login)\n$ module load singularity $ qsub -I -l select=1:mem=50GB:ncpus=10:ngpus=1 -l walltime=02:00:00 -q volta_login # walltime = 02:00:00 can be changed $ singularity exec /app1/common/singularity-img/3.0.0/tensorflow_2.3.1-cuda_11.1-cudnn_8-ubuntu18.04-py36-nvcr20.11.simg jupyter notebook Terminal 2\n login using port forward: ssh -L YYY:volta01:8888 nusip@atlas9  or atlas 8   open localhost:9999  ","permalink":"https://Vacationwkx.github.io/posts/ssh-jupyter/","summary":"This blog shows up a way to ssh a ubuntu computer (as Jupyter Notebook server) from mac.\nPrerequirements Ubuntu / Remote Setting SSH Setting run the following command to enable the ssh on ubuntu(18.04)\n$ sudo apt-get install openssh-server Jupyter Notebook Setting(optional) using jupyter notebook may require the token or password, you may refer to the jupyter notebook official site for details.\n  Token\ntake note of the resultjupyter notebook command, the token is inside the link after token=, e.","title":"Use Jupiter Notebook by ssh"},{"content":"In reinforcement learning, we can\u0026rsquo;t run infinite times to update the whole $Q$ - value table or $V$ - value table, efficient update choices must be made.\nGenerally thinking, which $s$ or $(s,a)$ has more opportunity to get $R$ (high value), should be updated more to converge to the optimal. But stochastic exploring is also required to jump out of sub-optimal. The simple idea to implement is $\\epsilon$ -greedy.\nTips:\nTo generate a list of probability of action set, which sum to 1, we can use Dirichlet distribution, e.g.\np=numpy.random.dirichlet(np.ones(n_actions)) $\\epsilon$ -Greedy $$ \\begin{aligned}\\max \u0026amp;: p=1-\\epsilon+\\frac{\\epsilon}{|A|}\\\\ \\mathrm{others}\u0026amp;: p=\\frac{\\epsilon}{|A|}\\end{aligned} $$\ni.e, random chose with $\\epsilon$ and chose the best with $1-\\epsilon$\nwith code\np[max_value_index]=1-epsilon p+=epsilon/n $\\epsilon$ - Soft the only requirement of $\\epsilon$ - Soft is each probability greater than $\\epsilon /|A|$, Dirichlet is useful here.\np=np.random.dirichlet(np.ones(n))*(1-epsilon) p+=epsilon/n ","permalink":"https://Vacationwkx.github.io/posts/rl-epsilon-policy/","summary":"In reinforcement learning, we can\u0026rsquo;t run infinite times to update the whole $Q$ - value table or $V$ - value table, efficient update choices must be made.\nGenerally thinking, which $s$ or $(s,a)$ has more opportunity to get $R$ (high value), should be updated more to converge to the optimal. But stochastic exploring is also required to jump out of sub-optimal. The simple idea to implement is $\\epsilon$ -greedy.\nTips:","title":"epsilon-Greedy vs epsilon-Soft"},{"content":"In this post, we will compare the state value update or state-action value update equation in fundamental rl methods.\nMonte Carlo $$V(s)\\leftarrow V(s)+\\frac{1}{N}[G_t-V(s)]$$\nrecall that, monte carlo update use average $V(s)=\\sum G_t/N$. After simple transformation, we will have the error form equation:point_up_2:.\nand the corresponding state-action value update is:\n$$Q(s,a)\\leftarrow Q(s,a)+\\frac{1}{N}[G_t-Q(s,a)]$$\nTemporal Difference TD is driven from $\\alpha$ - Monte Carlo, but replace the $G_t$ with $R_t+\\gamma V(s')$, since we want a instant update per step rather than till terminal.\n$$V(s)\\leftarrow V(s)+\\alpha [R_t + \\gamma V(s') -V(s)]$$\nThe state-action one is simply change the $V(s)$ to $Q(s,a)$, and becomes Sarsa (omitted).\nQ-learning Q-learning is a off-policy method, where the update ignores the policy while other on-policy only use the state-action value determinted by $\\pi(a|s)$. Recall that, even $s$ and $a$ are connected in $Q(s,a)$, but the $s'$ is determineted by environment, and $a$ is chosen by $\\pi(a|s)$.\n$$Q(s,a)\\leftarrow Q(s,a)+\\alpha[R_t+\\gamma Q_{max}(s',a')-Q(s,a)]$$\n","permalink":"https://Vacationwkx.github.io/posts/rl-update-equation/","summary":"In this post, we will compare the state value update or state-action value update equation in fundamental rl methods.\nMonte Carlo $$V(s)\\leftarrow V(s)+\\frac{1}{N}[G_t-V(s)]$$\nrecall that, monte carlo update use average $V(s)=\\sum G_t/N$. After simple transformation, we will have the error form equation:point_up_2:.\nand the corresponding state-action value update is:\n$$Q(s,a)\\leftarrow Q(s,a)+\\frac{1}{N}[G_t-Q(s,a)]$$\nTemporal Difference TD is driven from $\\alpha$ - Monte Carlo, but replace the $G_t$ with $R_t+\\gamma V(s')$, since we want a instant update per step rather than till terminal.","title":"Value Update Comparsion among Basical RL"},{"content":"Basic Recap Reinforcement learning bases on $V(s),Q(s,a),\\pi(a|s),R,G$:\n  $V(s)$ : state value, often used in model-based method;\n  $Q(s,a)$ : state-action value, often used in model-free method;\n why state-action: $s\\rightarrow a$ is defined partly in $\\pi(a|s)$, and $V(s,a),\\pi(a|s)$ are all parameters inside agent, consequently, $Q(s,a)$ is a combination of $V(s)$ and $\\pi(a|s)$.    $\\pi(a|s)$ : the policy of a agent, chose a $a$ (action) at a $s$ state;\n  $R$ : reward, got from each step\n  $G$ : a time-scale reward recording, or a estimate of value for current state. $$ G_t=R_T+\\gamma R_{T-1}+\\gamma^2R_{T-2}+\u0026hellip;=\\sum\\limits_{t+1}^{T}\\gamma^{T-i} R $$\n $T$ : Terminal time $\\gamma$ : a self-defined parameter to look how much further into future \u0026ndash; long future reward would not affect that much,but instant does. From the equation, the $G$ is influenced by $R$ and $\\gamma$, but for a well-behaved future-telling agent $\\gamma$ is usually set to 1or 0.9, which indicates, for a self-made envornment, $R$ should be set properly to obtain a wanted training result.    A little more from Basic Example : a hero in game, collects he always coins(reward) along a path in a 2d grid map to gain experience\n1\n  Real Existing Items:\nOnce the hero has the real items, it can absolutely get the max reward from environment.\n $G$ represents how many future values the position has, (even $\\gamma$ is also self-defined, but in my view, $\\gamma$ doesn\u0026rsquo;t affect that much.) and $R$ is what the hero gets from each step in the environment.    Esitimate:\nEstimate is what the hero guess about the $G$, which is $E(G)$. But obviously, in an environment, $G$ is related to state and time, when the hero is exploring with a policy. Then $E(G)$ should be $E_{\\pi}(G_t|S_t=s)$, that\u0026rsquo;s what we get from training.\n $v_{\\pi}(s)$ - value function $q_{\\pi}(s,a)$ - action-value function  These 2 are generally the same with $V(s),Q(s,a)$, since basically the policy always exist for most of the agent. The only difference is now they are estimate for $G$ with policy $\\pi$.\n  The FAMOUS Bellman Equation The Bellman equation is basiccally connecting the $v_{\\pi}(s)$ and $v_{\\pi}(s')$, or $q_{\\pi}(s,a)$ and $q_{\\pi}(s',a')$,\n2 $$ \\begin{aligned} v_{\\pi}(s)\u0026amp;=E_{\\pi}[G_t|S_t=s]\\\\\\\n\u0026amp;=E_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_t=s]\\\\\\\n\u0026amp;=?E_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_{t+1}=s']\\\\\\\n\u0026amp;=\\sum_{a\\in A}\\pi(a|s)\\sum_{s'\\in S}p(s',r|s,a) E_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_{t+1}=s']\\\\\\\n\u0026amp;=\\sum_{a\\in A}\\pi(a|s)\\sum_{s'\\in S}p(s',r|s,a) [r+\\gamma E_{\\pi}[G_{t+1}|S_{t+1}=s']\\\\\\\n\u0026amp;=\\sum_{a\\in A}\\pi(a|s)\\sum_{s'\\in S}p(s',r|s,a) [r+\\gamma v_{\\pi}(s')]\\\\\\\n\\end{aligned} $$\nSimilarily explained above, $E(G)$ will become $E_{\\pi}(G_t|S_t=s,A_t=a)$ for $q_{\\pi}(s,a)$, then the Bellman equation changes to:\n$$ \\begin{aligned} q_{\\pi}(s,a)\u0026amp;=E_{\\pi}[G_t|S_t=s,A_t=a]\\\\\\\n\u0026amp;=?E_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_{t+1}=s',A_{t+1}=a']\\\\\\\n\u0026amp;=\\sum_{s'\\in S}p(s',r|s,a)E_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_{t+1}=s']\\\\\\\n\u0026amp;=\\sum_{s'\\in S}p(s',r|s,a)\\sum_{a'\\in A}\\pi(a'|s')E_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_{t+1}=s',A_{t+1}=a']\\\\\\\n\u0026amp;=\\sum_{s'\\in S}p(s',r|s,a)\\sum_{a'\\in A}\\pi(a'|s')[r+\\gamma E_{\\pi}[G_{t+1}|S_{t+1}=s',A_{t+1}=a']]\\\\\\\n\u0026amp;=\\sum_{s'\\in S}p(s',r|s,a)\\sum_{a'\\in A}\\pi(a'|s')[r+\\gamma q_{\\pi}(s',a')]\\\\\\\n\\end{aligned} $$\nChoose Path based on Bellman Equation When the hero stand at $s$ state seeing all $v_{\\pi}(s')$ , but only one step will be chosen in reality, which means $\\pi(a|s)=1$ for this action $a$. This decision will let the $v_{\\pi}(s)$ biggest, and the policy will be updated and $v_*(s)$ is defined as: $$ \\begin{aligned} v_*(s)\u0026amp;=\\max_a \\pi(a|s)\\sum_{s'\\in S}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]\\\\\\\n\u0026amp;=\\sum_{s'\\in S}p(s',r|s,a_{\\max})[r+\\gamma v_{\\pi}(s')]\\\\\\\n\u0026amp;=q_{\\pi}(s,a_{\\max}) \\end{aligned} $$\n$p(s',r|s,a)$ is of course not controlled by the hero, thus, policy has the only option in next step \u0026ndash; at $s'$ choose $a'_{\\max}$ , where $q(s',a')$ is max for all $a' \\in A$. Use the same logic,\n$$ \\begin{aligned} q_*(s,a)\u0026amp;=\\sum_{s'\\in S}p(s',r|s,a)[r+\\gamma q(s',a'_{\\max})]\\\\\\\n\\end{aligned} $$\n$v_{\\pi}(s)$ vs $q_{\\pi}(s,a)$ $q_{\\pi}(s,a)$ seems have chosen the $a$ without policy. But thinking deeply, policy $\\pi(a|s)$ controls the choice when finally the hero acts in the environment. The $\\pi$ for $v(s)$ and $q(s,a)$ just dedicates the policy is updated according $v(s)$ and $q(s,a)$.\nNo matter which is used in policy update, what really matters is the next state $s'$, the $v(s')$ or the $\\sum\\limits_{s'\\in S} p(s',r|s,a)[r+\\gamma v(s')] $, since again the $p(s',r|s,a)$ is not controllable.\nOnce the next step is determinated, $a$ at this state $s$ is also confirmed. $q(s,a)$ just more connects to the next state.\n$v(s)$ choses the path by comparsion between multiple $v(s')$, but $q(s,a)$ indicates the path by comparsion between its company $q(s,a_1), q(s,a_2), q(s,a_3)\u0026hellip;$.\nUpdate Methods Clarification Monte Carlo, Temporal-Difference and Q-Learning are all model-free methods, which means the probability departing from states to states is unknown. The above optimal policy is used in Dynamic Programming, since the $p(s',r|s,a)$ is known. That\u0026rsquo;s also the reason why use DP in model-based environment. For model-free environment, the value is estimated by exploring and update. MC, TD or Q-learning just differ at these 2 processes.\nMonte Carlo The basic idea of Monte Carlo is to estimate value by : $$ V(s)=\\frac{G}{N} $$\nin the step update form: $$ V(s)\\leftarrow V(s)+\\frac{R-V(s)}{N} $$\nwith starting from $N=1$, in Monte Carlo $V(s)=G$.\nWith this setting, Monte Carlo performs the best with full exploring, also means $\\epsilon=1$ for on policy MC control with $\\epsilon$-soft algorithm, and must run enough steps, which is absolutely slow!!!\nUsing this idea, of course in most environments, exploring start with argmax at policy update will fail.\nNevertheless, the $G$ is driven from trajecotry $\\left[s_0,a_0,r_1,s_1,a_1,r_2,\u0026hellip;,s_{T-1},a_{T-1},r_T\\right]$ updated by $G_{s_t}=R_t+\\gamma G_{s_{t-1}}$, where the Terminal $G_{s_T}=0$. No terminal then no value update and policy update. However, a random walk can\u0026rsquo;t garante reaching the terminal.\n$\\alpha$-constant Monte Carlo The $\\alpha$ - constant Monte Carlo updates it by: $$ \\begin{aligned} V(s)\u0026amp;=V(s)+\\alpha [G-V(s)]\\\\\\\n\u0026amp;=V(s)+\\frac{G-V(s)}{\\frac{1}{\\alpha}}\\\\\\\n\u0026amp;=V(s)(1-\\alpha)+\\alpha G\\\\\\\n\\end{aligned} $$\nIn $\\alpha$ - constant will always consider part of the original $V(s)$: 3\n$$ \\begin{aligned} V_{ep+1}(s)\u0026amp;=[V_{ep-1}(s)(1-\\alpha)+\\alpha G_{ep-1}](1-\\alpha)+\\alpha G_{ep}\\\\\\\n\u0026amp;=V_{ep-1}(1-\\alpha)^2+\\alpha(1-\\alpha)G_{ep-1}+\\alpha G_{ep}\\\\\\\n\u0026amp;=V_1(1-\\alpha)^{ep}+\\sum_1^{ep}\\alpha(1-\\alpha)^iG_i\\\\\\\n\\end{aligned} $$\nfor $\\alpha \u0026lt;1$, when $t\\rightarrow \\infty$, $V_{\\infty}$ has more value depending on $G$, and specially recent $G$.\nWhat\u0026rsquo;s more, when updating the value, the value $V(s)$ is moving towards to the actual value, no matter is updated by Monte Carlo average method or TD or Q-learning, so partly we can trust the new $V(s)$.\nTemporal Difference TD is a bootstrapping method, which is quiet determined by the old value. $$ V_{ep+1}(s)=V_{ep}(s)+\\alpha[R+\\gamma V_{ep}(s')-V_{ep}(s)] $$\nComparing with the $\\alpha$ - constant Monte Carlo $V_{ep+1}(s)=V_{ep}(s)+\\alpha [R_{ep}+\\gamma G_{ep-1}-V_{ep}(s)]$, $\\alpha$ is the stepsize and also determines the update quantity of the $V(s)$. Once $V(s')$ is estimated close to the real value, $V(s)$ is updated by one step closer to the real $V(s)$. Digging to the end, the terminal $V(s_T)=0$, and the $V(s_{T-1})$ s are all updated exactlly by one step close to the real value, unlike the Monte Carlo, always needing a trajectory to end to update the value.\nFor TD, update is not deserved with end to terminal. The first run to terminal is only updated valuable on the $V(s_{T-1})$, and next run is $V(s_{T-2})$, and so on\u0026hellip;\nOn one side, the $V(s)$ is updated truely along the way to terminal, with this chosen path, the value is updated more fast, since the agent prefers to go this path under $\\epsilon$ - greedy policy; On the other side, with randomly exploring, the agent searchs for a better way to terminal. Once found the new path will be compared with the old one, the $V(s)$ will determine the optimal path.\nIf we use $Q(s,a)$ in TD, then the algorithm is called the famous sarsa. $$ Q_{ep+1}(s,a)=Q_{ep}(s,a)+\\alpha[R+\\gamma Q_{ep}(s',a')-Q_{ep}(s,a)] $$ Similarly, the $Q(s,a)$ is updated from the $Q(s_T,a_T)$ once reaches the terminal.\nQ-learning While the agent is still randomly walking in the environment without arriving at the terminal, then the updated value is equavalent to random initialized $Q(s,a)$. The meaningful value is like TD starting from $Q(s_T,a_T)$, the difference locates at that, because of the continous exploring, we can safely choose the best way with fast speed. This indicates we can determine the best step from state $s$ by looking into the $Q(s',a')$s and gives the $s-1$ a symbol ($Q(s,a)$) that $s$ is the best within his company: $$ Q_{ep+1}(s,a)=Q_{ep}(s,a)+\\alpha [R+\\gamma Q_{ep}(s',a'_{\\max})-Q_{ep}(s,a)] $$ Gradually the from the starting state, the agent find the fast by seeing the biggest $Q(s,a)$ at each state.\nOther Thinking Arbitrary Initial Q or V Even give $Q(s,a)$ or $V(s)$ a positive value at start, by updating, a negative value $Q(s',a')$ or $V(s')$ will contribute part of it. At least the $R$ will definately affect negatively to it. After this, a positive $Q(s,a)$ or $V(s)$ can\u0026rsquo;t be compared with a $Q(s,a)$, which is driven from the positive value given by terminal.\nWhere goes $p(s',r|s,a)$ ? When we have the model, then $p(s',r|s,a)$ can help us compare the $V(s)$ by avioding the low value and passing more though the high value, or directly getting more rewards. In model free, there is no $p(s',r|s,a)$ in offer. But no matter $p(s',r|s,a)$ or $Q(s,a)$ or $V(s)$ just to find the best way. With many exploring, the value is showing the best probability of getting best reward, then there is no need to setting $p(s',r|s,a)$ in model free environment.\n$p(s',r|s,a)$ is of course not controlled by the hero.\n  Online image from here \u0026#x21a9;\u0026#xfe0e;\n Sutton\u0026rsquo;s Reinforcement Book \u0026#x21a9;\u0026#xfe0e;\n The ep represents the episode number, there we use first visit Monte Calro method. \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://Vacationwkx.github.io/posts/rl-comparsion/","summary":"Basic Recap Reinforcement learning bases on $V(s),Q(s,a),\\pi(a|s),R,G$:\n  $V(s)$ : state value, often used in model-based method;\n  $Q(s,a)$ : state-action value, often used in model-free method;\n why state-action: $s\\rightarrow a$ is defined partly in $\\pi(a|s)$, and $V(s,a),\\pi(a|s)$ are all parameters inside agent, consequently, $Q(s,a)$ is a combination of $V(s)$ and $\\pi(a|s)$.    $\\pi(a|s)$ : the policy of a agent, chose a $a$ (action) at a $s$ state;","title":"Monte Carlo vs TD vs Q-learning"},{"content":"Updated on 2021.04.21\nMy Steps to HUGO  Follow the official website basic installation; Find a great (better stable) theme at official theme library Config the config.yml or config.toml by instruction from the theme producer.  something you may want to pay attention to images In hugos, the post use image from static folder, you can insert a image in a markdown method:\n![some caption](/static/image.jpg) ![some caption](/static/folder/image2.jpg) In this condition the folder is like:\ncontents | | | --- posts | | | ---- example.md static | --- folder | | | ---- image2.jpg ---- image.jpg youtube A embedded youtube video in post can be simple add as a shortcode:\n {{\u0026lt; youtube w7Ft2ymGmfc \u0026gt;}} For a youtube video from link https://www.youtube.com/watch?v=w7Ft2ymGmfc\n if you also want to display shortcode in .md check this website from Chris Liatas.  Theme Configuration - hugo-PaperMod Most of the parameter can be changed simply in config.yml or config.toml, like title, author. To use emoji in the config.yml go straight to the emoji website, find one :smile: with code U+1F604 , replace the +1 with 000, namely insert \\U000F604.\nPS: in the .md file, :simle: is a emoji, but the \\U000F604 is also d\nArchive / Tags To tiggle the archive and tags do cost me some time. The answer is in the config.yml for the theme temple website, where\nmenu: main: - name: Archives url: /archives/ weight: 5 - name: Tags url: /tags/ weight: 10 taxonomies: category: categories tag: tags series: series ... is needed to add at the end of your config.yml.\nfavicon.png Chose the logo you want to use, go to favicon.io to generate the icon, replace the ones in the themes/hugo-PaperMod/static.\nIn case you find no changes, may be the browser still has cache, clear it or switch another one will work.\nAdding more\u0026hellip; Mathjax Only simple 3 changing separately in css, layout/.html and head.html or header.html. The tutorial you can find here.\nif you find multiline flattened into one line (check this issue ) change all \\\\ at end to \\\\\\\\\\\\.\nSmall problems fix:  infinity symbol in MathJax $\\infty$ code is \\infty .  [New] Hugo 0.82.0 has been a long time not update my blog, life got boring, but i barely realized it, until recent. Refeeling the vivid daily time is long lasting, but i did it again.\nSo, new hugo add some more settings, like description in front matters. We can have a auto github page with the Github Action said in the official doc. The magic code will build a branch in the github repository called gh-pages.\nAll you should do is go to Setting \u0026raquo; Pages \u0026raquo; Source and change the branch to gh-pages.\nThe paperMod theme i used also updated. You can find here. Following some new instructions i rebuilt my blog, add search and comment block.\n","permalink":"https://Vacationwkx.github.io/posts/new-on-hugo/","summary":"Updated on 2021.04.21\nMy Steps to HUGO  Follow the official website basic installation; Find a great (better stable) theme at official theme library Config the config.yml or config.toml by instruction from the theme producer.  something you may want to pay attention to images In hugos, the post use image from static folder, you can insert a image in a markdown method:\n![some caption](/static/image.jpg) ![some caption](/static/folder/image2.jpg) In this condition the folder is like:","title":"New on Hugo🤺"},{"content":"Note of learning creating Blog in Jekyll\nGuide / Tutorial For Jekyll  Quick Start - Official Tutorial Official one is the most basis and reliabel. What is Jekyll? and How it works? - Jekyll Bootstrap Better first go though the official one to understand what is Variables Front Matter .. Sinple guides of jekyll from CarlBoettiger Pure Guide, what do first, what next. Fast guide for those, who know htmlCloudCannon  For HTML  No Base No Building: What is HTML / HTML5 und CSS Basic Knowledge in Mozilla Basic Example \u0026amp; Code Training HTML - W3School and JS -W3School  Front Matter --- title: Antigragile - Book Review layout: post author: Vac categories: [Book] tags: [Probability] --- Front Matter default default your font matters in _config.yml. for all the file(path: \u0026quot;\u0026quot;) in _posts(type: \u0026quot;posts\u0026quot;), set front matter values: layout to post, author to Vac, excerpt_separator to (normally the first paragraph).\ndefaults: - scope: path: \u0026quot;\u0026quot; type: \u0026quot;posts\u0026quot; values: layout: \u0026quot;post\u0026quot; author: \u0026quot;Vac\u0026quot; excerpt_separator: \u0026lt;!-- more --\u0026gt; Tags more tags can be listed like:\ntags: - tag1 - tag2 to make sure it works, use 2 spaces before the subtags Also, you can add properties to the subtags:\ntags: - name: tag1 type: math - name: tag2 type: art and cite it in liquid with:\n {% raw %} {% for tag in page.tags %} {{ tag.name }}, {{ tag.type }} {% endfor %} {% endraw %} Markdown Index The homepage of the site, accepting front matter\nImage {% highlight markdown %} ![caption of image](/assest/path/to/image.jpg){: width=\u0026quot;50%\u0026quot;} {% endhighlight %}  {: class=} can add any css to the image part no need for plug-in  Code   Way 1\n{% raw %} {% hightlight [code type]%} some code here. {% endhighlight %} {% endraw %}   Way 2 - markdown type\n{% highlight markdown %} ` ` `[code type] ` ` ` {% endhighlight %}   when liquid wrongly translate your code to error\n{\u0026amp;#37; raw \u0026amp;#37;} your code is safe now.{\u0026amp;#37; endraw \u0026amp;#37;}\n  Link to Post {% raw %} [say something]({% post_url 2019-11-2-name-of-it %}) {% endraw %}  no type at end  Emoji recommend website Full Emoji List, or you can just google one\n find your one in the list; get the UTF-8 code like U+1F642; wirte direct in format \u0026amp;#x1F642;; and it will be like 🙂.  Liquid 2 types\n  {% raw %} {{ page.title | upcase }} {% endraw %}   {% raw %} {% if ... %} {% %} {% endraw %}   Include + Parameters you can add a new layout as a common used piece in the blog for example, insite a youtube video:\n  in the layout file youtube.html use include.parameter:\n{% highlight markdown %} \u0026lt;div class=\u0026quot;spacing youtube\u0026quot;\u0026gt; \u0026lt;iframe width=\u0026quot;560\u0026quot; height=\u0026quot;315\u0026quot; src=\u0026quot;https://www.youtube.com/embed/{{ include.youtube_id }}\u0026quot; frameborder=\u0026quot;0\u0026quot; allowfullscreen\u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; {% endhighlight %}   in the blog add the parameter after the include:\n{% raw %} {% include youtube.html youtube_id=\u0026quot;YIiHiMXOeYU\u0026quot; %} {% endraw %}   ","permalink":"https://Vacationwkx.github.io/posts/learn-jekyll/","summary":"Note of learning creating Blog in Jekyll\nGuide / Tutorial For Jekyll  Quick Start - Official Tutorial Official one is the most basis and reliabel. What is Jekyll? and How it works? - Jekyll Bootstrap Better first go though the official one to understand what is Variables Front Matter .. Sinple guides of jekyll from CarlBoettiger Pure Guide, what do first, what next. Fast guide for those, who know htmlCloudCannon  For HTML  No Base No Building: What is HTML / HTML5 und CSS Basic Knowledge in Mozilla Basic Example \u0026amp; Code Training HTML - W3School and JS -W3School  Front Matter --- title: Antigragile - Book Review layout: post author: Vac categories: [Book] tags: [Probability] --- Front Matter default default your font matters in _config.","title":"Note Learn Jekyll"},{"content":"Personal daily thinking splitted by categories.\nThoughts this year 懶社會 2019.4.13 社會價值觀的單壹變態到理所當然了，錢能做到壹切，沒有錢什麽都困難重重。懶勁和怕麻煩如此，不惜不去反思這樣的道路是否可行合理。人的因素被大大弱化，只是眾多經驗的合集，固然各個人的不同努力被忽視只能得出錢的重要性。\n2019.4.7 極端主義、完美主義和懶都有聯系，不願去接受世界上的多樣和復雜化，費腦子。和以神經網絡擼所有模型沒有差別，與不想思考更多的可能性壹樣，與人物單壹臉譜化類似\n教育和個人發展 2019.4.10 虛假的公平是不是殘忍，教會妳做夢，卻沒有揭示每個人的能力不壹，實現的難度根本不能比\n教育是階級保質。對於階級較低，卻仍然把大部分希望投建於教育，無異於耗本賭博，和籌碼豐厚且熟悉遊戲規則的人對賭，且賭局壓力延伸到作為底牌的孩子上。應試教育進壹步模糊了籌碼顯示表。降低門檻，是開始行動的小秘訣。\n跑步不用專門天氣不熱不涼，壹身裝備齊全，水備好，包背上，毛巾帶上。想就去做就行了，其他的都是提高舒適感的。現在不想學習，先看看雜誌，靜下來進入持續專註狀態，看到壹定時間就有想去學的心。狀態的突變阻力很大，壹步步轉換讓自己也更容易接受。\n2019.4.2 打破馬太效應的關鍵在於如何界定優勢劣勢。若是以正態分布來看，大多數是出於中間地帶，比上為馬太劣勢比下馬太優勢。如果以二八定律，妳總能在八裏面找到自己二的位置。所以很簡單，自信壹點，認定自己處於馬太優勢，充分利用壹些資源，以榨幹的形式。服務界教育界，購買的不僅是產品，也有人際關系，前者理所當然，後者購買附送的潛在價值。機構如大學，越龐大越復雜潛在價值越多，不僅是人更有很多平臺。（公司待考\n2019.3.28 作為人就是註定普通，有對手和同類就是妳的非唯壹體現，而唯壹是壟斷的，極易被新產生的對手削弱唯壹性的，而無法長久（相對於人的生命）\n語言和信息表達 2019.4.7 語言和詞語作為交流的工具，為了使大家都能理解且快速理解，免不了使用固定的代指。對於實體的代指往往沒有爭議，如蘋果，對於抽象詞語，比如帶有社會屬性的男人女人，或者長大好看的人，往往有以大部份代小部分或者是以舊定義為主，也就是這部分詞語意思也會隨著社會而變化。 固然人如何使用詞語和語言種類無關，重要的是內容的表達。對於詞語不同的理解也可以通過進壹步拆分內涵來化解矛盾歧義，如男性是生理還是心理上的。\n2019.3.28 對於單個觀點很容易被攻擊，但作為系統推出卻很容易被忽視，而當作大整體的壹部分被容納\n真實 2019.4.16 新聞的反轉引發人們的懷疑，沖擊漫畫敷衍式政府報告\n2019.4.4 史記跨越千年真的沒有壹點偏移嗎？即便有不同史料相互佐證，但每個撰寫者都是司馬遷嗎？不排除群體有私下討論的可能嗎？從單個撰寫者來說，容易陷入幸存者偏差，如果不是四處行走，壞的不會抹去但可能簡述，美好也可能被誇張\n慣性思維和平等 2019.4.4 關於政治正確，是否是我們被填鴨慣了，主觀否認了對方由內到外的真心的關註。\n2019.3.28 女性的物化隨著擴大宗族部落的需求而發展也隨著新型社會的建立而廢除。人作為生命體，雖然神聖，但作為社會計劃目標壹部分，免不了納入規劃，而易被物化。\n群體和個性 2019.4.13 關於反大眾，大家都說好的，妳能說出好在哪裏，好的地方和別人的有什麽不同，能具體化和其他同類對比種種，而不是大家都說好，妳就說不好，妳不好的地方就在於別人都說他好\n2019.4.4 群體成立的根本在於至少持有同壹觀點。理解力參差不壹，為了向同壹觀點妥協，必然向最低看齊群體間恐怖和謠言的發布可以從源頭滅絕，以發現惡性變異體並排出群體之外的方式。即妳背叛了國軍，不配擁有軍裝\n2019.3.25 如果大群體是需要智慧的，那麽群體內必須有階級之分，舍棄低等群體的直接話語權也是必然\n旅行如果是由他人的感受起意，這樣的感覺很難在自己的旅途中重現，即妳看到的旅行日記沒有任何主觀上的參考價值，加工後的照片也帶著拍攝者的理解和表達\n2019.3.23 絕對中庸等於毫無意義，最好的是在各種群體裏旗幟鮮明的表達自己，不管是正向10%還是負向100%\n社會發展 2019.4.2 由維生物質產品工廠演變為科技代碼產品工廠關於整治、環保，都是now or never 的問題\n2019.3.30 受制於記者來報道，何不人人記者，記者也是人身前往記錄，身處於這個地方直接發表不是更好。缺乏的是平臺，任何壹個平臺如果有實體，也會被打擊，人腦網絡就是趨勢\n2019.3.28 人治是需要物質前提的，但達到物質富裕的標準是模糊的，不界定清楚便是永久地排擠和壓迫\nOld thoughts from Weibo Year 2018 2018-2-5 除了免除自身騷擾，舉報的人和打小報告的心理壹不壹樣呢？\n2018-1-30 網絡對自主獨立性考驗很大啊，人越來越像，長的化的妝，發型穿著。真的stand alone complex了嗎？\n2018-1-21 半智能化時代馬上到來，最致命的就是藏在技術背後的人際關系網了。AI執政的那壹天我肯定是看不到了，羨慕啊\n2018-1-21 其實我很是希望泡泡破掉，這樣很多都會連環崩壞，重新洗牌\n2018-1-11 大家都是審判管，殺紅了眼。 這樣的人為什麽不去死，曝光他讓他工作丟掉。 做了這種事，才判20年，為什麽不死刑，監獄條件那麽好，怎麽會悔改。 這種明星怎麽能登上臺，封殺他，永遠不能讓他再出名。 真是優秀嚴格的民族，對自己也這麽嚴格，怪不得那麽多抑郁，自己犯了錯就內心封殺自己，就判了死刑。\n2018-1-11 遺忘是否是必然，我從書上看到焚書抗儒、竇娥冤、長城白骨，觸動沒有各種新聞實事來的劇烈，可能也就是哀嘆壹聲。佩服感同身受的人。 此處大膽推論，我若不是少數，不管多久，不管再怎麽努力，現在的很多都會遺忘。\n2018-1-11 Heterosexual對homosexual攻擊點之壹：妳孩子是同性戀妳怎麽辦 homo估計都覺得這點不要太好笑，hetero很多卻認為這很難辦。 別人是不是和自己沒有什麽關系，什麽搶了資源，自己沒機會什麽的太容易扳倒了。 自己孩子是不是其實也和父母沒有什麽關系，延不延續後代是孩子的決定，要把自己必須延續後代的意誌預先加到孩子身上，這點才很奇怪吧。也有說homo沒法生小孩，孩子不是親生什麽的，可以百度壹下。 世界真奇妙\nYear 2017 2017-12-21 壹出生就加入了名為生產力的大軍，怎麽會有人權呢\n2017-11-12 我不敢保證我會做得對 生命和正義是矛盾體\n2017-10-4 妳以為在創造歷史，不過是在重蹈覆轍\n2017-10-4 當國家運作起來像個公司，處處盈利，生下來也就成了終身奴役。 妳幹什麽買什麽吃什麽受什麽服務，都是公司旗下。 妳只能不停幹，才不會被他開除。 哪怕妳只是在裏面端茶遞水，妳也會以它世界500強的地位自豪。\n2017-9-3 壹直追求純粹，追求完美有錯嗎？沒錯，但這是最高目標不是最低目標。 社會和諧美好好不好，是不是只能有好。公眾人物正不正，還是只能正只能上帝。 道家壹生二，二生三，黑色和白色壹直共存，白的有多白黑的就有多黑，這才是世界。 犯了錯能不能原諒，到底看做了什麽還是改正了什麽，又或者說壹旦錯了，就失去了所有的信任和權利，壹輩子低聲下氣。 這壹點和伊斯蘭教有點類似但底線不壹樣：無論錯多大都可以原諒。 是否可以按嚴重程度來看懲罰的時間和原諒與否容易發現中國人可以特別記仇，國仇家恨，恨了也只是生氣，自己該怎麽活，買什麽還是買，吃什麽還是吃，世界也還是轉。\n2017-8-18 德國又在報道我國不平衡問題了。少數民族，偏遠山區，讀書貧窮。 出來這麽久，每每看到還是能壹把拉進沈重的現實，就是幾座山的距離，也是天上地下。 每天為利益錙銖必較，對著地鐵某某人偏離了排隊線兩厘米發微博舉報，形式主義還要居上風多久。 我們要做的還有很多。每每有人看到已有的成效誇誇其談，也是莫名的自豪吧。我們現有的成績，都是人民奮鬥出來的。 壹生過得這麽苦，也就成了歷史書上壹句，領導人英明的決定。 曾經的人民為大也越來越少提及，不忘初心，初心又是新定義的。 加油吧，別躁。\n2017-8-14 勿忘國恥不是為了恨別人，而是再出發，說白了也是恨自己。\n2017-8-7 墻的作用都是差不多的，比如柏林墻\n2017-7-23 死了仿佛才能給這個世界帶來點什麽 死了的罪犯，給受害者家屬帶來安慰，給社會帶來和諧，於是充斥殺人償命，死刑極 刑，只需更改定義就能讓刀子變成聖物 死了的藝術家，給作品帶來新生，給商家帶來利益，所以我們都在哀嚎痛心，悔恨以 往的疏漏，仿佛這些能讓他復活能再創造幾幅剛剛升值的畫作 只希望歷史不死\n2017-7-21 網上的分歧點主要在於 說國家的不好是忘恩負義還是合理提議 說實話誰不希望自己國家好 國家包含了政治概念，但不全是 或許是不想做眼蝦耳聾的信徒，但又懶得翻梵語寫的教義，還是多看多問少吵架\n2017-7-20 世界總是充滿仇恨，為了生命又去犧牲生命\n2017-7-13 個人對科學的定義是，利用壹定規律對未來壹定時間內的壹定程度上預測 對於鄭測實施（主要是鈣隔）應不應該存在打壹眼看壹下孔，打的不好再換個地方打的試探性舉措，是否可以說這完全沒有將科學運用到徹底，還是說我們的社會科學並不能達到向前推進壹大截的程度，那既然如此為何老止步不前和以前的方法並無二樣 口口聲聲說的信奉科學，或許也像其他話壹樣浮在表面\n2017-6-1 少數服從多數到底啥道理\nYear 2016 2016-11-27 平等意味著平庸，或許我不是壹個有趣的人\n2016-6-17 以前人們會尾隨喜歡的人來接近ta，現在只需要點壹個follow就好。以前要偷偷了解壹 個人需要拍照打聽竊聽器啥的，現在就翻翻朋友圈和微博就知道中午在哪吃了啥喜歡 哪個明星平時和誰壹起玩最近在忙啥。似乎唯壹不同的就是後者更容易造假。\n","permalink":"https://Vacationwkx.github.io/posts/thoughts-in-2019/","summary":"Personal daily thinking splitted by categories.\nThoughts this year 懶社會 2019.4.13 社會價值觀的單壹變態到理所當然了，錢能做到壹切，沒有錢什麽都困難重重。懶勁和怕麻煩如此，不惜不去反思這樣的道路是否可行合理。人的因素被大大弱化，只是眾多經驗的合集，固然各個人的不同努力被忽視只能得出錢的重要性。\n2019.4.7 極端主義、完美主義和懶都有聯系，不願去接受世界上的多樣和復雜化，費腦子。和以神經網絡擼所有模型沒有差別，與不想思考更多的可能性壹樣，與人物單壹臉譜化類似\n教育和個人發展 2019.4.10 虛假的公平是不是殘忍，教會妳做夢，卻沒有揭示每個人的能力不壹，實現的難度根本不能比\n教育是階級保質。對於階級較低，卻仍然把大部分希望投建於教育，無異於耗本賭博，和籌碼豐厚且熟悉遊戲規則的人對賭，且賭局壓力延伸到作為底牌的孩子上。應試教育進壹步模糊了籌碼顯示表。降低門檻，是開始行動的小秘訣。\n跑步不用專門天氣不熱不涼，壹身裝備齊全，水備好，包背上，毛巾帶上。想就去做就行了，其他的都是提高舒適感的。現在不想學習，先看看雜誌，靜下來進入持續專註狀態，看到壹定時間就有想去學的心。狀態的突變阻力很大，壹步步轉換讓自己也更容易接受。\n2019.4.2 打破馬太效應的關鍵在於如何界定優勢劣勢。若是以正態分布來看，大多數是出於中間地帶，比上為馬太劣勢比下馬太優勢。如果以二八定律，妳總能在八裏面找到自己二的位置。所以很簡單，自信壹點，認定自己處於馬太優勢，充分利用壹些資源，以榨幹的形式。服務界教育界，購買的不僅是產品，也有人際關系，前者理所當然，後者購買附送的潛在價值。機構如大學，越龐大越復雜潛在價值越多，不僅是人更有很多平臺。（公司待考\n2019.3.28 作為人就是註定普通，有對手和同類就是妳的非唯壹體現，而唯壹是壟斷的，極易被新產生的對手削弱唯壹性的，而無法長久（相對於人的生命）\n語言和信息表達 2019.4.7 語言和詞語作為交流的工具，為了使大家都能理解且快速理解，免不了使用固定的代指。對於實體的代指往往沒有爭議，如蘋果，對於抽象詞語，比如帶有社會屬性的男人女人，或者長大好看的人，往往有以大部份代小部分或者是以舊定義為主，也就是這部分詞語意思也會隨著社會而變化。 固然人如何使用詞語和語言種類無關，重要的是內容的表達。對於詞語不同的理解也可以通過進壹步拆分內涵來化解矛盾歧義，如男性是生理還是心理上的。\n2019.3.28 對於單個觀點很容易被攻擊，但作為系統推出卻很容易被忽視，而當作大整體的壹部分被容納\n真實 2019.4.16 新聞的反轉引發人們的懷疑，沖擊漫畫敷衍式政府報告\n2019.4.4 史記跨越千年真的沒有壹點偏移嗎？即便有不同史料相互佐證，但每個撰寫者都是司馬遷嗎？不排除群體有私下討論的可能嗎？從單個撰寫者來說，容易陷入幸存者偏差，如果不是四處行走，壞的不會抹去但可能簡述，美好也可能被誇張\n慣性思維和平等 2019.4.4 關於政治正確，是否是我們被填鴨慣了，主觀否認了對方由內到外的真心的關註。\n2019.3.28 女性的物化隨著擴大宗族部落的需求而發展也隨著新型社會的建立而廢除。人作為生命體，雖然神聖，但作為社會計劃目標壹部分，免不了納入規劃，而易被物化。\n群體和個性 2019.4.13 關於反大眾，大家都說好的，妳能說出好在哪裏，好的地方和別人的有什麽不同，能具體化和其他同類對比種種，而不是大家都說好，妳就說不好，妳不好的地方就在於別人都說他好\n2019.4.4 群體成立的根本在於至少持有同壹觀點。理解力參差不壹，為了向同壹觀點妥協，必然向最低看齊群體間恐怖和謠言的發布可以從源頭滅絕，以發現惡性變異體並排出群體之外的方式。即妳背叛了國軍，不配擁有軍裝\n2019.3.25 如果大群體是需要智慧的，那麽群體內必須有階級之分，舍棄低等群體的直接話語權也是必然\n旅行如果是由他人的感受起意，這樣的感覺很難在自己的旅途中重現，即妳看到的旅行日記沒有任何主觀上的參考價值，加工後的照片也帶著拍攝者的理解和表達\n2019.3.23 絕對中庸等於毫無意義，最好的是在各種群體裏旗幟鮮明的表達自己，不管是正向10%還是負向100%\n社會發展 2019.4.2 由維生物質產品工廠演變為科技代碼產品工廠關於整治、環保，都是now or never 的問題\n2019.3.30 受制於記者來報道，何不人人記者，記者也是人身前往記錄，身處於這個地方直接發表不是更好。缺乏的是平臺，任何壹個平臺如果有實體，也會被打擊，人腦網絡就是趨勢\n2019.3.28 人治是需要物質前提的，但達到物質富裕的標準是模糊的，不界定清楚便是永久地排擠和壓迫\nOld thoughts from Weibo Year 2018 2018-2-5 除了免除自身騷擾，舉報的人和打小報告的心理壹不壹樣呢？\n2018-1-30 網絡對自主獨立性考驗很大啊，人越來越像，長的化的妝，發型穿著。真的stand alone complex了嗎？","title":"Thoughts in 2019"},{"content":"A Chinese version book note, mostly cited directly from the book.\n  群體只有很普通的品質。\n群體只有很普通的智慧。\n群體也只有最基本的智能。\n群體同時也只具有最低甚至更低層次的智力。\n  數量在人類社會中會經常性地產生壹種充足的理由。\n  群體的疊加只是愚蠢的疊加，而真正的智慧卻被愚蠢的洪流湮沒了。\n  但是群體則不然，群體不需要承擔任何責任，群體就是法律，群體就是道\n  德，群體的行為天然就是合理的。\n這是因為單獨的壹個人是有其名姓的，而群體的本身就是它的名字。\n群體是無名氏！\n無名氏不需要為他所做的任何事情承擔責任。\n因為無名，所以無由指控。\n  這是壹種接近於迷信的狀態，事實上，人類歷史上出現的所有迷信者，莫不處於壹種缺乏認知的無意識狀態。\n  擁有獨立意識時的膽小鬼，在群體中會變成壹個膽大包天、肆意妄為的人。\n  擁有獨立意識時的老實人，在群體中會表現得蔑視法律、我行我素。\n  群體比個人更有力量，但是群體的表現是極不穩定的；而個人無論是智力還是能力方面，總是維持在壹個平均的水平線上的。\n  群體能夠幹出什麽來，取決於影響群體的暗示具有何種性質。如果這種性質是積極的、進步的、有意義的，那麽群體的表現就會是相應的積極進步而有益。反之，如果主宰群體行為的暗示是負面的心理能量，那麽群體的表現就會非常可怕——如果把群體比作是同壹個人，那麽這種主宰群體行為的暗示力量就好比是人的思想，如果這個人的思想是善良的，那麽這個人必然是善良的，反之亦然。\n  關於這種英雄主義精神——我們經常見到的這種令人無限景仰的利他主義行為，赴湯蹈火，慨然就義，為壹種教義或是觀念，而將個人的生死置之度外，或是願意以自己的生命為代價而追求它的純潔性。這種情況最為常見的發生於壹個群體之中，壹個孤零零的個人願意為了某壹教義付諸壹切的事情，在現實中是很難見到的。\n  現代心理學認為：類同於群體表現的低級進化形態的生命，主要是以女性、野蠻的原始人以及兒童為主。\n妳終於憤怒了，是不是？\n然而本書將不會理睬妳的憤怒——也就是說，本書將拒絕為這壹觀點作出解釋或辯護。如果妳需要知道究竟的話，不妨去翻看壹些更為專業的心理學論述。\n Is there some problems of you? 找打呢？\n   對於壹個群體來說，它從壹個極端到另壹個極端，是輕而易舉的事情，這期間的變化如行雲流水般的自然，對於群體來說幾乎不是什麽難事。\n  因為早期的基督徒認為哲學和科學研究都是異教徒的活動，應該被禁止\n  ","permalink":"https://Vacationwkx.github.io/posts/the-crowd/","summary":"A Chinese version book note, mostly cited directly from the book.\n  群體只有很普通的品質。\n群體只有很普通的智慧。\n群體也只有最基本的智能。\n群體同時也只具有最低甚至更低層次的智力。\n  數量在人類社會中會經常性地產生壹種充足的理由。\n  群體的疊加只是愚蠢的疊加，而真正的智慧卻被愚蠢的洪流湮沒了。\n  但是群體則不然，群體不需要承擔任何責任，群體就是法律，群體就是道\n  德，群體的行為天然就是合理的。\n這是因為單獨的壹個人是有其名姓的，而群體的本身就是它的名字。\n群體是無名氏！\n無名氏不需要為他所做的任何事情承擔責任。\n因為無名，所以無由指控。\n  這是壹種接近於迷信的狀態，事實上，人類歷史上出現的所有迷信者，莫不處於壹種缺乏認知的無意識狀態。\n  擁有獨立意識時的膽小鬼，在群體中會變成壹個膽大包天、肆意妄為的人。\n  擁有獨立意識時的老實人，在群體中會表現得蔑視法律、我行我素。\n  群體比個人更有力量，但是群體的表現是極不穩定的；而個人無論是智力還是能力方面，總是維持在壹個平均的水平線上的。\n  群體能夠幹出什麽來，取決於影響群體的暗示具有何種性質。如果這種性質是積極的、進步的、有意義的，那麽群體的表現就會是相應的積極進步而有益。反之，如果主宰群體行為的暗示是負面的心理能量，那麽群體的表現就會非常可怕——如果把群體比作是同壹個人，那麽這種主宰群體行為的暗示力量就好比是人的思想，如果這個人的思想是善良的，那麽這個人必然是善良的，反之亦然。\n  關於這種英雄主義精神——我們經常見到的這種令人無限景仰的利他主義行為，赴湯蹈火，慨然就義，為壹種教義或是觀念，而將個人的生死置之度外，或是願意以自己的生命為代價而追求它的純潔性。這種情況最為常見的發生於壹個群體之中，壹個孤零零的個人願意為了某壹教義付諸壹切的事情，在現實中是很難見到的。\n  現代心理學認為：類同於群體表現的低級進化形態的生命，主要是以女性、野蠻的原始人以及兒童為主。\n妳終於憤怒了，是不是？\n然而本書將不會理睬妳的憤怒——也就是說，本書將拒絕為這壹觀點作出解釋或辯護。如果妳需要知道究竟的話，不妨去翻看壹些更為專業的心理學論述。\n Is there some problems of you? 找打呢？\n   對於壹個群體來說，它從壹個極端到另壹個極端，是輕而易舉的事情，這期間的變化如行雲流水般的自然，對於群體來說幾乎不是什麽難事。\n  因為早期的基督徒認為哲學和科學研究都是異教徒的活動，應該被禁止","title":"[Book]The Crowd - A Study of the Popular Mind"},{"content":"personal experience of gre.\nVerbal Reasoning Good video suggested from Youtube: GRE Verbal Section Walkthrough: How I take the test (Part 1)\n   等號 強詞 逆向思維 浮動思維  Verbal Time Arrangement  word,2 min short essay :1.5:1 middle essay: 2.5:1 long essay: 4.5:1  閱讀   內容\n 3s  句子 段 篇   句間關系  同 反  but, yet,however,nevertheless 換對象 否定含義單詞       criticize, oppose, deny, reject\noverestimate, challenge, object\nunlikely, problem, objection\n  問題 同義改寫  Analytical Wirting Download book: 新GRE寫作5.5\nIssue Article Structure Positive-Negative-Sum\nTypes of the questions  Cause-effect  if cause right? right to deduce to effect?   Suggestion  Capability? viable? come up to a reverse result? alternative?   Is or not  standard ? find a definition is correctable   Defination  is the core? redefine   Facts  public knowing to reverse result? reverse example    Types of Demonstration  Reason : pure logical Example  with result reverse exact example   Definition : pure logical  ","permalink":"https://Vacationwkx.github.io/posts/gre/","summary":"personal experience of gre.\nVerbal Reasoning Good video suggested from Youtube: GRE Verbal Section Walkthrough: How I take the test (Part 1)\n   等號 強詞 逆向思維 浮動思維  Verbal Time Arrangement  word,2 min short essay :1.5:1 middle essay: 2.5:1 long essay: 4.5:1  閱讀   內容\n 3s  句子 段 篇   句間關系  同 反  but, yet,however,nevertheless 換對象 否定含義單詞       criticize, oppose, deny, reject","title":"Personal GRE Summary"},{"content":" A chinese vision book note.\n Foreword   從隨機事件（或壹定沖擊）中獲得的有利結果大於不利結果的就是反脆弱的，反之則是脆弱的\n  人造的復雜系統往往會引發失控的連鎖反應，它會減少甚至消除可預測性，並導致特大事件。因此，現代世界的技術性知識可能會不斷地增加，但矛盾的是，它也會使事情變得更加不可預測。現在，由於人為因素的增加，以及我們逐漸地遠離了先祖和自然的模式，加上林林總總的設計復雜性削弱了強韌性，以至於“黑天鵝”的影響在進壹步增加。此外，我們成為壹種新型疾病的受害者，即新事物狂熱癥，它使我們建立起面對“黑天鵝”事件時會表現得極其脆弱的系統，卻自以為實現了所謂的“進步”。\n  當妳尋求秩序，妳得到的不過是表面的秩序；而當妳擁抱隨機性，妳卻能把握秩序、掌控局面\n  “黑天鵝”問題有壹個惱人方面，實際上也是壹個很核心的並在很大程度上被忽略的問題，即罕見事件的發生概率根本是不可計算的\n  脆弱推手往往陷入蘇聯–哈佛派謬見，即（不科學地）高估科學知識的能量。秉持這種謬見的人就是所謂的天真的理性主義者、合理化者，或有時被叫作合理理性主義者，因為他認為事情背後的原因是可以自動顯現的。我們不要混淆**“合理化”與“理性”**這兩個概念——它們幾乎完全**相反**。\n  2. 隨處可見的過度補償和過度反應   巴爾紮克曾講述女星如何賄賂記者（常用實物賄賂）來撰寫吹捧她們的評論，但聰明的女星往往讓記者寫些負面評論，因為這會讓觀眾對她們更有興趣。\n Negative anecdote arrests people and will be superseded by good repute.\n   很多人，像偉大的羅馬政治家監察官老加圖[4]，就將安逸——幾乎任何形式的安逸——視為通向墮落的道路。他不喜歡所有輕易就能獲得的東西，因為他擔心這樣會削弱意誌。\n 深有體會\n   而且，他擔心這種弱化意誌的事件影響的不只是個人層面：事實上，整個社會都會墮落。想想看，當我在寫這些文字時，我們正生活在債務危機中。整個世界空前富裕，也承受著空前沈重的債務，靠舉債生活。歷史記錄表明，對社會來說，我們越富有，就越難量入為出。富足比貧困更難對付。\n  2011年的海嘯中，經歷災難性事故的福島核反應堆又是另壹個例證。它是以能承受歷史上最強震級的地震為標準建造的，建造者並沒有想過更糟的情況，也沒過想過歷史上那次最糟的地震本身也是突如其來、沒有先例的。\n 黑天鵝事件的毀滅性\n   那些了解生物領域細菌耐藥性的人，卻完全不理解塞內加在《寬恕》壹書中就處罰的反效應所寫下的格言。他寫道：“重復處罰雖然打擊了某些人的仇恨，卻激起了所有人的仇恨……就像樹木修剪後將再抽出無數新的枝條壹樣。”事實上，革命正是在壓迫中孕育的，殺害幾名示威者只會讓更多的人站起來反抗。壹首愛爾蘭的革命歌曲就飽含了這層寓意：\n妳的堡壘築得越高，我們就越有力量。\n  政治運動和叛亂具有高度的反脆弱性，愚蠢的行為就是試圖用暴力壓制它們，而不是想辦法操控它們、以退為進，或找到更精明的策略，就像赫拉克勒斯殺死九頭蛇怪壹樣。\n  信息是具有反脆弱性的，湮滅信息的努力比宣傳信息的努力更能增強信息的力量。壹個典型的例子是，許多人越是為自己辯解，越會越描越黑。\n  告訴別人壹個秘密，並強調說這是壹個秘密，懇請對方“千萬不要告訴任何人”，妳越是強調這是壹個秘密，它傳播得就越快\n  “我的兒子，我對妳很失望，”他說，“我從未聽到外界對妳的指責。妳已經證明了自己根本無法激發別人對妳的嫉妒。”\n  如果說藝術家具有反脆弱性，那麽底層勞動人民就具有強韌性，而申請過抵押貸款的銀行中層員工則是脆弱性的極致代表。\n  3.貓與洗衣機   他更傾向於將經濟視為有機體，只是缺乏壹個理論框架來進行恰當的表達。因為亞當·斯密深諳復雜系統的不透明性，以及系統內各部分的相互依存關系，否則，他不可能發明“看不見的手”這壹概念。\n  我們時常聽到有人喊“誰在統治我們”。就好像這個世界真的需要有人來統治壹樣。\n 無政府主義萬歲\n   如果妳不是壹臺洗衣機或壹座布谷鳥鐘，換句話說，如果妳還活著，妳的內心深處就會喜歡壹定程度的隨機性和混亂。\n  4. 殺死我的東西卻讓其他人更強大   此外，我們在之前的內容中提到了「犧牲」。非常遺憾，自己的錯誤往往只會讓他人或集體受益，好像個人天生就該為了更崇高的利益而非自己的利益犯錯。\n  如果我們將歷史視為類似於自然的復雜系統，那麽，我們將看到，與自然壹樣，它不會讓某個帝國永遠統治這個星球——即使從巴比倫王國、埃及王國到波斯王國再到羅馬王國，每壹個超級大國都相信自己的統治將長盛不衰，並讓歷史學家將這壹結論載入史冊。\n  讓我們來看看抗生素耐藥性的現象。妳越是努力殺滅細菌，幸存的細菌就越頑強——除非妳能夠完全消滅它們。\n  癌癥治療也是壹樣：能夠在化療和放療後生存下來的癌細胞往往繁殖得更快，並占據那些較弱癌細胞被殺死後留下的空白。\n  以群體而非個體的眼光看事物，以及「有利於後者的必然有害於前者」\n  我們往往會從別人的錯誤中受益——遺憾的是，受益人不是那些犯錯誤的人。\n  福島核危機的故事也是壹樣的：我們可以肯定地說，它讓我們覺察到了核反應堆的問題（以及小概率事件的威力），避免了更大的災難。\n  犯罪的人要比那些從來沒犯過罪的人更可靠。犯了很多錯誤（當然，同樣的錯誤不會犯壹次以上）的人要比那些從來沒有犯過錯的人更可靠。\n  較高層級事物的反脆弱性有賴於較低層級事物的脆弱性，或者較低層級事物的犧牲。\n  殺不死我的，並未使我變得更堅強，但它讓我幸存下來，因為我比別人更強壯；由於它殺死了別人，也就是消滅了弱者，我們種群的當前平均素質變強了\n  如果不打破個體的利益，整個經濟體就無法生存；壹味地保護是有害的，為了個體的利益制約進化的力量似乎毫無必要\n  但這個世界上存在著成功的胡說八道者、成功的偽哲學家、成功的評論員、成功的顧問、成功的說客，或成功的商學院教授，而他們根本不承擔個人風險。\n  我夢想的解決方案是，我們設立壹個美國創業者日，並為其寫就以下感謝詞：\n 妳們中的大多數人將遭受失敗、輕慢和貧困，但是我們非常感謝妳們為了全球經濟的增長與他人脫貧而承擔的風險和做出的犧牲。妳們正是反脆弱性的來源。美國感謝妳。\n   5. 露天市場與辦公樓   “小”在其他許多方面都表現出了壹種美。事實上，“小”匯總起來（也就是小單元的集合）比“大”更具反脆弱性——事實上，大的東西註定要分崩離析，這是壹個數學屬性，稍後我們還會解釋，但可悲的是，這種現象似乎普遍見於大公司、大型哺乳動物以及大政府。\n 大小是相對的，沒有什麽是穩定的，只有永恒的變化。若說由小的不斷聚集起來的整體是穩定的，那麽這個整體又可以組成另壹個大的整體，那相對小的整體便是不穩的了。因此穩定也是相對的，需考慮在同壹個刺激或波動下。\n  同時，沒有壹個國家從古至今依然存在，但他的子民卻生生不息。\n  如果有壹天，壹個絕對壹致的存在出現，他壹定強過宇宙。\n   讓–雅克·盧梭引用他的話語寫道：“馬基雅維利寫道，在謀殺和內戰中，我們的共和國更為強盛，而公民也學到了美德……微小的騷動和焦慮滋養了靈魂，讓物種繁榮的不是和平，而是自由。”\n  有些人陷入了幼稚的“火雞式”思維，認為這個世界變得越來越安全了，並天真地把它歸功於神聖的“國家”（盡管自下而上管理的瑞士才是全球暴力發生率最低的地方）。它好比是說，核彈更安全，因為它們爆炸的機會較小。世界各地的暴力行為越來越少，但是戰爭的潛在危害卻更大了。\n  8. 預測是現代化的產物  在“黑天鵝”領域內，總是有知識所無法達到的極限，無論統計學和風險管理科學發展到如何復雜的程度  9.胖子托尼和脆弱推手  在尼羅的眼裏，沒有比過度精致更可怕的了——無論是衣服、食物、生活方式還是舉止，而且財富是非線性的。錢壹旦超過了壹定數量，就會將人們的生活無限復雜化，讓我們不得不憂慮我們在某國房產的看守人是否玩忽職守，以及惹上諸多隨著財富增長而成倍增加的麻煩。  11. 千萬別嫁給搖滾明星   社會政策往往保護弱勢群體，同時讓強者各盡其職，而不會幫助中間階層鞏固其特權，因為這樣會阻礙進化，造成各種經濟問題，最終還會給窮人帶來最大的傷害。\n  更多的杠鈴策略：做壹些瘋狂的事情（偶爾砸壞家具），就像希臘人在飲酒討論會進行至後半場時所表現的那樣，而在更大的決策上保持“理智”。閱讀無用的娛樂雜誌，以及經典書籍或復雜的著作，但不要讀平庸的書籍。與大學生、出租車司機和園丁，或最優秀的學者交流，但不要和庸庸碌碌但野心不小的學者交流。如果妳不喜歡某個人，要麽隨他去，要麽擊垮他，不要只是停留於口頭攻擊。\n  隨機性的杠鈴策略會通過減輕脆弱性、消除傷害導致的不利風險來增強反脆弱性，也就是減少不利事件帶來的痛苦，同時確保獲得潛在收益。\n  最優化的策略是每周戒酒3天（給肝臟壹定的休息時間），而在其他4天的時間內自由飲酒。\n  那位電腦創業者史蒂夫·喬布斯的力量正是在於不信任市場研究和焦點小組——這些都基於問人們他們想要什麽——而是跟隨自己的想象。他的理念是，人們根本不知道他們想要什麽，直到妳提供給他們。\n  可選擇性可以帶我們去很多地方，從根本來說，選擇權能讓妳具備反脆弱性，它幫助妳從不確定性的積極面中受益，同時也不會因其消極面而經受嚴重的傷害。\n 缺乏選擇權，導致脆弱，不得不做\n   與工業革命中的英國壹樣，美國的資產很簡單，就是在冒險和運用可選擇性方面，這是壹種卓越的能力，即參與到合理的試錯活動中，失敗了也不覺得恥辱，而是重新來過，再次失敗，再次重來。而現代日本則恰好相反，失敗給人帶來恥辱，導致人們想方設法地隱藏風險，不管是金融風險還是核電風險；創造很小的收益，卻要坐在火藥桶上，這種態度與他們尊敬失敗英雄的傳統，以及雖敗猶榮的觀念，形成了奇怪的對比。\n 都是假的，都是在騙我，曾經的過錯，還有多久才覺悟\n   12. 泰勒斯的甜葡萄   即我們以為靠我們的技能成就的許多東西其實大多來自選擇權，而且是被妥善運用的選擇權，很像泰勒斯的案例，也很像自然選擇的情況，而不能歸功於我們自認為掌握的知識\n  如果妳認為是教育為妳帶來了財富，而不認為教育是財富的結果，或者認為明智的行為和發現是明智的思想的結果，那麽妳壹定會大吃壹驚。讓我們來看看這是壹種什麽樣的驚奇。\n 我已經大吃壹驚了\n   13.教鳥兒如何飛行   然而，人們卻繼續關註精氣和體液，醫生繼續開出靜脈抽血（放血術）、灌腸劑（對此我不想多作解釋）、泥糊劑（在發炎的組織上敷上壹塊潮濕的面包或麥片粥）等藥方，如此又延續了幾個世紀，直到巴斯德研究所，提出證據表明細菌是導致這些感染性疾病的罪魁禍首。\n you know too much\n   然而，如果舊的技術不僅不自然，而且明顯有害，或者向新技術轉換（比如帶滾輪的旅行箱）顯然能消除舊技術的副作用，那麽死守著過時的技術就是徹底的非理性了。\n 顯性多害，深層下難以道明，以人類的知識，只能在淺薄下討論，因而壹直縫縫補補，新的問題不斷出現，又不斷開發手段解決新的問題\n   有些事情需要毀滅才能促進整個系統的改善，這被稱為創造性破壞\n  我們很少有這樣的錯覺，即看到男生大多留短發就認為頭發的長度決定性別，或者戴上領帶就能成為壹名商人。但是，我們卻很容易陷入其他的副現象，特別是當壹個人被淹沒在新聞驅動的文化中時。\n 我國並不少見，被認錯多次\n   15. 失敗者撰寫的歷史   不，我們並不是把理論付諸實踐。我們是在實踐中創造出理論。\n 事實上是兩者都有，沒有絕對的絕對，科學在於傳承這些無意間的發現，並作到想學的人就能學懂和運用，妳可用可不用，可學可不學。\n   在文藝復興之前，數學只用於智力測驗。\n   文藝復興後相隔萬裏的中小學也是。\n   妳可以列壹個藥物清單，看看有多少藥物是通過“黑天鵝”方式誕生的，有多少是按設計研發出來的。\n 科研的懷疑和嚴謹極容易打壓任何壹個偶然發現，畢竟在今日科技水平下，任何錯誤都可能都可能導致人類滅絕。研發核裂變和幾世紀前解剖人類屍體的代價完全不可比擬。但沒有科學式的敏感，只依靠靈感，也是難以發現黑天鵝。 平衡的科學發現就是如此，難以壹直得利。所謂的得利，也只是損失的轉移。\n   事物的邏輯在我們的掌控之外（在上帝、自然或自發力量的手中）\n 可憐的人類壹直是發現者，照搬壹切。\n   16.混亂中的秩序   人工飼養的獅子壽命更長，從技術上說，它們更加富有，工作更有保障，如果這些是妳所註重的生活標準的話……\n 單調的重復還耗費資源，不如在睡夢中死去\n   實際上，我的父親壹直在向我暗示好成績所帶來的問題：當時他們班上成績最差的壹個學生（具有諷刺意味的是，剛好是我在沃頓商學院的同學的父親）結果成了壹個白手起家的商人，是迄今為止他們班上最成功的人（他有壹個巨大的遊艇，上面醒目地刻著他的名字的首字母）；還有壹個差生靠從非洲進口木材大賺了壹筆，40歲前就退休了，成為壹個業余的歷史學家（主要研究古地中海的歷史），並進入政界。雖然我的父親似乎並不重視教育，但卻重視文化或金錢——他鼓勵我去尋求這兩樣東西（我最初尋求的是文化）。\n Stay Hungry\n   我認為，這與那些被選為尖子生，只努力在少數科目上取得高分，而不是按照自己的興趣愛好發展的人是壹樣的：只要讓他們稍微遠離壹下他們的研究領域，妳就能看到他們如何喪失信心和壹味抗拒。\n 被英文和德文雙重打擊的我，自學起了日文\u0026hellip;\n   壹旦我看某本書或研究某個課題時感覺無趣，我就轉移到下壹個目標，而不是完全放棄閱讀——當妳只讀學校教材時，壹旦覺得無趣，妳會有壹種完全放棄的念頭，什麽也不想做，或者幹脆逃學來讓自己擺脫這種沮喪。所以，我的訣竅是，妳可以厭倦讀壹本特定的書，但不要厭倦閱讀的行為。這樣，妳所閱讀並消化的知識才能快速增長。隨後，妳就會毫不費力地發現，書中自有黃金屋，就像理智但無設定方向的試錯研究所能帶來的。這與選擇權相同，失敗了也不要停滯不前，必要時向其他方向摸索，跟隨那種廣闊的自由感和機會主義的引領，試錯就是壹種自由。\n  來看看壹個不是書呆子的人是怎樣應用數學的：首先，發現問題；其次，找出適用的數學解決方案（就像人習得語言壹樣）。而不是在真空中通過研究定理和虛擬的例子去學習，然後，改變現實，讓它看起來像那些例題。\n 試錯壹直有效和理論壹直有效沒有什麽差別，都是臺球桌上某個範圍內的球進，每個人的觀念形成都存在幸存者偏差，屬於當時環境下的可行性有效驗證，即美國模式中國照搬會死的很慘\n   17.胖子托尼與蘇格拉底辯論  事實上，在生活中最嚴重的錯誤莫過於將不可敘述的事物誤認為是愚蠢的東西 蘇格拉底排斥寫作的原因是，壹旦寫下文字，這些內容就定型了、不可改變了，而實際上對他來說，答案從來不是最終的和固定的。 問題的答案永遠是根植於問題之中的；千萬不要直接回答壹個對妳來說毫無意義的問題。 胖子托尼說：“我親愛的蘇格拉底……妳知道他們為什麽要把妳處死嗎？那是因為妳讓人覺得盲目跟隨習慣、本能和傳統是愚蠢的事。妳有時可能是正確的，但是，妳可能會讓他們對自己壹直做得很好和並未陷入麻煩的事情感到疑惑。妳正在摧毀人們對自己的理解。妳拿我們對某些事情的無知來取樂。而且，妳沒有答案可以給他們。” “有些事情我不理解未必表示我無知”，這也許是尼采所在的時代最有力的壹句話——我們在序言中曾說過與之類似的話，在界定那些將不理解的事物誤當作不合理的事物的脆弱推手時。 而這個觀點正是尼采所斥責的：知識是萬能的，錯誤是邪惡的，因此科學是壹個讓人感到樂觀的產業。這種科學樂觀主義的說法激怒了尼采：這等於是利用推理和知識來為烏托邦效力。 在對阿威羅伊的抨擊中，他表達了壹個著名的觀點，即從定義上說，邏輯排除了細節，而由於真相只存在於細節中，因此，“在尋找道德和政治科學中的真相時”，邏輯只是“無用的工具”。  18. 壹塊大石頭與壹千顆小石頭的區別   歐洲的機場和鐵路負荷都很重，因為它們似乎過於高效了。它們以接近最大容量的負荷來運行，導致冗余和閑置容量很小，因此成本很低；但是，只要乘客數量稍微增加，比如由於壹個小小的乘客滯留問題導致航班增開5%，就會給機場造成混亂，乃至讓怨聲載道的旅客在機場過夜，唯壹的安慰就是聽壹些流浪者用吉他演奏法國民歌。\n 他就是19世紀法國的思想家歐內斯特·勒南\n   古人是如何看待天真的理性主義的：天真的理性主義削弱而非促進了思想，因此只會帶來脆弱性。他們了解，壹知半解壹定有危險。\n  哈佛大學就像是路易·威登包或卡地亞手表壹樣。中產階層的父母背上了沈重的負擔，把儲蓄中越來越大的份額送入這些機構，也就是把他們的錢轉移給了行政管理人員、房地產開發商、教授，以及其他機構。在美國，越來越多的學生貸款自動轉移給這些“抽租者”。在某種程度上，這與詐騙沒有區別：人們需要壹個“名牌”大學來給自己鍍金；但是我們知道，集體社會不是靠有組織的教育來推進的，情況正好相反。\n  我的答案是：騙局永遠是脆弱的。哪壹個騙局在歷史上能夠永遠持續下去？時間和歷史終將揭穿脆弱性的真面目，對此我很有信心。教育是壹個持續膨脹而不受外部壓力約束的機構，終有壹天它會崩潰。\n  20. 時間與脆弱性   學術工作因為有尋求關註的傾向，所以很容易受制於林迪效應：想想看，數以百萬計的論文不管在出版時如何大肆宣傳，本質上也只是噪聲。\n  盡量不要讀過去20年裏出版的書，除了不是寫過去50年內歷史的歷史書。\n  今天的大型公司到那時應該都消失了，因為它們將規模視為自己的實力，結果卻被規模所誤：規模之所以是公司的敵人，是因為它會導致公司在“黑天鵝”面前呈現不相稱的脆弱性。\n clever jack ma\n   21.醫療、凸性和不透明  為什麽過去幾千年來我們人類的皮膚壹直暴露於陽光下，現如今卻突然需要防曬了，是不是因為大氣變化了，曬太陽變得對我們有害了呢？還是因為如今人類生存的環境與皮膚的色素不相匹配了，或者更確切地說，是防曬產品的制造商需要賺取利潤呢？  23. 切身利害：反脆弱性和犧牲他人的可選擇性   拉爾夫·納德有壹條簡單的準則：對戰爭投贊成票的人至少有壹個後代（子輩或孫輩）參加戰爭。\n 臺灣武力回收說\n   24. 給職業帶上倫理光環   其次，在壹個復雜系統中，法律法規的字面意思與實質意思之間的差異很難讓人辨識。也就是說，技術性的、復雜的、非線性的環境比只涉及少數變量的線性環境更容易受人控制\n  “缺乏證明某事存在的證據”與“證明某事不存在的證據”混為壹談的現象\n 壹片叫好，報喜不報憂\n   他們只會詭辯，卻絲毫沒有接近真相\n  ","permalink":"https://Vacationwkx.github.io/posts/antifragile/","summary":"A chinese vision book note.\n Foreword   從隨機事件（或壹定沖擊）中獲得的有利結果大於不利結果的就是反脆弱的，反之則是脆弱的\n  人造的復雜系統往往會引發失控的連鎖反應，它會減少甚至消除可預測性，並導致特大事件。因此，現代世界的技術性知識可能會不斷地增加，但矛盾的是，它也會使事情變得更加不可預測。現在，由於人為因素的增加，以及我們逐漸地遠離了先祖和自然的模式，加上林林總總的設計復雜性削弱了強韌性，以至於“黑天鵝”的影響在進壹步增加。此外，我們成為壹種新型疾病的受害者，即新事物狂熱癥，它使我們建立起面對“黑天鵝”事件時會表現得極其脆弱的系統，卻自以為實現了所謂的“進步”。\n  當妳尋求秩序，妳得到的不過是表面的秩序；而當妳擁抱隨機性，妳卻能把握秩序、掌控局面\n  “黑天鵝”問題有壹個惱人方面，實際上也是壹個很核心的並在很大程度上被忽略的問題，即罕見事件的發生概率根本是不可計算的\n  脆弱推手往往陷入蘇聯–哈佛派謬見，即（不科學地）高估科學知識的能量。秉持這種謬見的人就是所謂的天真的理性主義者、合理化者，或有時被叫作合理理性主義者，因為他認為事情背後的原因是可以自動顯現的。我們不要混淆**“合理化”與“理性”**這兩個概念——它們幾乎完全**相反**。\n  2. 隨處可見的過度補償和過度反應   巴爾紮克曾講述女星如何賄賂記者（常用實物賄賂）來撰寫吹捧她們的評論，但聰明的女星往往讓記者寫些負面評論，因為這會讓觀眾對她們更有興趣。\n Negative anecdote arrests people and will be superseded by good repute.\n   很多人，像偉大的羅馬政治家監察官老加圖[4]，就將安逸——幾乎任何形式的安逸——視為通向墮落的道路。他不喜歡所有輕易就能獲得的東西，因為他擔心這樣會削弱意誌。\n 深有體會\n   而且，他擔心這種弱化意誌的事件影響的不只是個人層面：事實上，整個社會都會墮落。想想看，當我在寫這些文字時，我們正生活在債務危機中。整個世界空前富裕，也承受著空前沈重的債務，靠舉債生活。歷史記錄表明，對社會來說，我們越富有，就越難量入為出。富足比貧困更難對付。\n  2011年的海嘯中，經歷災難性事故的福島核反應堆又是另壹個例證。它是以能承受歷史上最強震級的地震為標準建造的，建造者並沒有想過更糟的情況，也沒過想過歷史上那次最糟的地震本身也是突如其來、沒有先例的。\n 黑天鵝事件的毀滅性\n   那些了解生物領域細菌耐藥性的人，卻完全不理解塞內加在《寬恕》壹書中就處罰的反效應所寫下的格言。他寫道：“重復處罰雖然打擊了某些人的仇恨，卻激起了所有人的仇恨……就像樹木修剪後將再抽出無數新的枝條壹樣。”事實上，革命正是在壓迫中孕育的，殺害幾名示威者只會讓更多的人站起來反抗。壹首愛爾蘭的革命歌曲就飽含了這層寓意：\n妳的堡壘築得越高，我們就越有力量。\n  政治運動和叛亂具有高度的反脆弱性，愚蠢的行為就是試圖用暴力壓制它們，而不是想辦法操控它們、以退為進，或找到更精明的策略，就像赫拉克勒斯殺死九頭蛇怪壹樣。\n  信息是具有反脆弱性的，湮滅信息的努力比宣傳信息的努力更能增強信息的力量。壹個典型的例子是，許多人越是為自己辯解，越會越描越黑。\n  告訴別人壹個秘密，並強調說這是壹個秘密，懇請對方“千萬不要告訴任何人”，妳越是強調這是壹個秘密，它傳播得就越快","title":"[Book]Antigragile"},{"content":" 游记 上\n 青旅有常住客，人很和善，本以為都是如我壹般的，其他人似乎都是結伴而行，有的看著比我小，但似乎也是大學生，但也有十分社會。各位都是夜貓子，十二點都還開著燈，床簾惘然虛設。我對光可謂十分敏，有點過強，就不要想睡了。無奈掙紮到1、2點，爬起來關了大燈，才睡去。要知道這個時候已經設置好第二天的起床鬧鐘了，還能睡3個半小時吧。睡前搜索了看日出的景點，講了好幾個，觀景臺和大石頭，還有若幹等等，兩個月過去實在是記不清了。初步定在某壹個近壹點的地方，看了看地點，騎自行車過去便趕得上，畢竟前壹晚騎了不下4次，周圍早已輕車熟路。\n第二天鬧鐘壹響就起，旅店內壹人沒有，就我壹人。說起來有個尷尬的事，我忘了帶牙膏，只有牙刷。前壹晚回來的晚，沒處去買，早上起得又早，店也沒開門，就幹刷牙。長教訓了！\n下來到前臺，就只有我壹人的腳步聲。前臺也只有壹個人，和昨日不壹樣，昨天的小妹應該是去睡覺了。退了房，把東西寄存在小隔間，拿上相機和手機就出發了，當然還有錢。其實錢並不是必須，杭州在馬爸爸的鼎力幫助下，誰誰都有二維碼。\n退過房後，出門鼓樓門下吃碗餛飩，10元壹大碗，想想昨晚也是他家的臭豆腐，有種大仇已報的感覺。其間上網，臨時更改地點，決定去大石處，和觀景臺於同壹處，兩百米之隔。\n可壹搜索，路遠了壹倍，騎自行車是來不及，當時已是五點多，日出應是六點多壹點。像餛飩店老板打聽，他們也支支吾吾說不出個什麽名堂。心壹橫決定去打的。\n來到路口，淩晨的車屈指可數，只有暗黃的燈光。來往的出租都是有客，等了近十分鐘，眼看時間壹點點被蠶食。靈機壹動，又開壹輛ofo來到昨日河坊街口（這兩日的ofo使用量支撐了這半年的使用量）。等了十分鐘不到，經過了好幾輛有客，被打走兩輛，終於坐上了車。\n司機不知道我的目的地，只有給他現開導航，很是神奇。送到地方，據稱是個山腳，上山路在黑暗裏都走了老半天。路窄，太陽還沒出來，周圍壹片安靜，就只有樹葉和水的聲音。眼看時間快到，只有壹個勁網上跑。此處路還錯綜復雜，大抵是周圍農舍緊鄰山腳，就開了很多路。\n上山路上壹個人沒有，僅有壹位掃地阿姨。仔細想來，當時那麽黑，阿姨真的掃的幹凈嗎？同時蘋果地圖坑的要死，讓我在路口打轉，還是憑著直覺上了山。不過杭州山低，跑個十分鐘就是平緩的路，應該是快到山頂了，見到壹些晨練的老人。此時天已有些發白，不敢放慢，還是向上趕路。\n快到巨石處，路不好走，仿佛長時間沒有人修繕，都是雨水風化的痕跡。巨石向下看就是山腳，壹個踩滑可能就順著石頭溜出去了，不是掉進西湖，就是葬身山谷。狼狽的我手腳並用，往上擡頭，已有兩人影立在巨石壹尖。此處和路有段距離，還不是很好爬上，看他們都裝備齊全，大包小包，也驚嘆他們是怎麽爬上此處。\n我本來在壹旁聽他們吹牛，細節忘了，大概是壹個是外地人，開著房車，老婆在車裏睡覺，他就上來拍。另壹個是當地的，老婆在家裏睡覺，他就爬起來拍。此處簡稱AB兩人。A經常和好友開著房車出去玩，壹路玩壹路拍，感覺好像不用上班，估計是個老總，長得小個，有點像馬雲。B兒子在美國留學歸來，在上海工作，和室友合租，壹個月房租幾萬，他戲稱：“兒子說要在上海買房，我給不起，但杭州幾百萬我還是可以的”。我禮貌地回應，沒有將心裏的mmp表達出來，白眼也只在肚子裏翻滾了。\n等著時間已經7點，天已經基本白亮，也不見太陽，看來是雲層太厚，沒法看見了。遠處是星星點點的城市人造光，但也是有奇妙的美感。\n見到壹處開闊地，周圍陸續有大石，恩還有壹只狗。狗甚是靈活，山石其間上竄下跳，不亦樂乎。\n PS : 現在整理，已有壹年之隔，記不清了，棄坑 😂\n ","permalink":"https://Vacationwkx.github.io/posts/tour2/","summary":" 游记 上\n 青旅有常住客，人很和善，本以為都是如我壹般的，其他人似乎都是結伴而行，有的看著比我小，但似乎也是大學生，但也有十分社會。各位都是夜貓子，十二點都還開著燈，床簾惘然虛設。我對光可謂十分敏，有點過強，就不要想睡了。無奈掙紮到1、2點，爬起來關了大燈，才睡去。要知道這個時候已經設置好第二天的起床鬧鐘了，還能睡3個半小時吧。睡前搜索了看日出的景點，講了好幾個，觀景臺和大石頭，還有若幹等等，兩個月過去實在是記不清了。初步定在某壹個近壹點的地方，看了看地點，騎自行車過去便趕得上，畢竟前壹晚騎了不下4次，周圍早已輕車熟路。\n第二天鬧鐘壹響就起，旅店內壹人沒有，就我壹人。說起來有個尷尬的事，我忘了帶牙膏，只有牙刷。前壹晚回來的晚，沒處去買，早上起得又早，店也沒開門，就幹刷牙。長教訓了！\n下來到前臺，就只有我壹人的腳步聲。前臺也只有壹個人，和昨日不壹樣，昨天的小妹應該是去睡覺了。退了房，把東西寄存在小隔間，拿上相機和手機就出發了，當然還有錢。其實錢並不是必須，杭州在馬爸爸的鼎力幫助下，誰誰都有二維碼。\n退過房後，出門鼓樓門下吃碗餛飩，10元壹大碗，想想昨晚也是他家的臭豆腐，有種大仇已報的感覺。其間上網，臨時更改地點，決定去大石處，和觀景臺於同壹處，兩百米之隔。\n可壹搜索，路遠了壹倍，騎自行車是來不及，當時已是五點多，日出應是六點多壹點。像餛飩店老板打聽，他們也支支吾吾說不出個什麽名堂。心壹橫決定去打的。\n來到路口，淩晨的車屈指可數，只有暗黃的燈光。來往的出租都是有客，等了近十分鐘，眼看時間壹點點被蠶食。靈機壹動，又開壹輛ofo來到昨日河坊街口（這兩日的ofo使用量支撐了這半年的使用量）。等了十分鐘不到，經過了好幾輛有客，被打走兩輛，終於坐上了車。\n司機不知道我的目的地，只有給他現開導航，很是神奇。送到地方，據稱是個山腳，上山路在黑暗裏都走了老半天。路窄，太陽還沒出來，周圍壹片安靜，就只有樹葉和水的聲音。眼看時間快到，只有壹個勁網上跑。此處路還錯綜復雜，大抵是周圍農舍緊鄰山腳，就開了很多路。\n上山路上壹個人沒有，僅有壹位掃地阿姨。仔細想來，當時那麽黑，阿姨真的掃的幹凈嗎？同時蘋果地圖坑的要死，讓我在路口打轉，還是憑著直覺上了山。不過杭州山低，跑個十分鐘就是平緩的路，應該是快到山頂了，見到壹些晨練的老人。此時天已有些發白，不敢放慢，還是向上趕路。\n快到巨石處，路不好走，仿佛長時間沒有人修繕，都是雨水風化的痕跡。巨石向下看就是山腳，壹個踩滑可能就順著石頭溜出去了，不是掉進西湖，就是葬身山谷。狼狽的我手腳並用，往上擡頭，已有兩人影立在巨石壹尖。此處和路有段距離，還不是很好爬上，看他們都裝備齊全，大包小包，也驚嘆他們是怎麽爬上此處。\n我本來在壹旁聽他們吹牛，細節忘了，大概是壹個是外地人，開著房車，老婆在車裏睡覺，他就上來拍。另壹個是當地的，老婆在家裏睡覺，他就爬起來拍。此處簡稱AB兩人。A經常和好友開著房車出去玩，壹路玩壹路拍，感覺好像不用上班，估計是個老總，長得小個，有點像馬雲。B兒子在美國留學歸來，在上海工作，和室友合租，壹個月房租幾萬，他戲稱：“兒子說要在上海買房，我給不起，但杭州幾百萬我還是可以的”。我禮貌地回應，沒有將心裏的mmp表達出來，白眼也只在肚子裏翻滾了。\n等著時間已經7點，天已經基本白亮，也不見太陽，看來是雲層太厚，沒法看見了。遠處是星星點點的城市人造光，但也是有奇妙的美感。\n見到壹處開闊地，周圍陸續有大石，恩還有壹只狗。狗甚是靈活，山石其間上竄下跳，不亦樂乎。\n PS : 現在整理，已有壹年之隔，記不清了，棄坑 😂\n ","title":"2017-12-01(2)-Not a serious Diary"},{"content":" 游记 下\n 今日正經地寫壹篇遊記，紀念這次突然起意卻又收獲頗豐的匆匆旅行。\n先列舉壹些第壹次：\n 第壹次壹個人出遊； 第壹次在火車站“裝逼”用電腦，但是真正的用了，翻譯了文檔，效率奇高； 第壹次在青旅還在小小學習了壹下，我承認，這是真裝逼； 杭州大叔大伯都迅速準確地稱呼了我，小姑娘難得，倒是壹個外國人尊敬地說了“Excuse me，sir” …  再說壹些原來，這個純粹是我見識淺薄，大家不要見笑：\n 原來白素貞是仙女下凡，並非僅是妖怪，被徐克老怪坑害不淺； 原來法海才是妖怪，為王八精； 原來斷橋不是斷的，路鋪的好得很，都能跑車了； 原來蘇堤和白堤是蘇軾和白居易的堤，相比蘇堤的安恬，白堤意外的熱鬧，詳見後敘； 原來杭州美女也不溫婉，是特別社會、時尚的，現代化的女性更多； …  現在，讓我們開始倒帶回憶： 上完課，本想和老師討論討論，於3:08緊急接到師傅通知，讓我立馬出發。似乎開了壹個壞頭，整個旅程也是匆匆而行。\n到站後，便開始了電腦“裝逼”，只見我嫻熟地掏出電腦，嫻熟地用指紋開了電腦(看看這嘴臉，現在還得意呢)。然後開始了頭痛的論文翻譯校對，這壹弄，頭也昏了，腰也酸了，得虧吃了麥當勞緩了過來（這下不得不提杭州站的肯德基，效率低，服務人員也少，簡直不像省會）。候車的座位旁邊壹家商店恰好有壹套小的便攜電子折疊鼓，先後有好幾個人來打了，都是胡亂打，我這個破水平都聽得出來，店員稍微好壹點，有點節奏感。不過，後來來了壹位大俠，神乎其技地演奏了壹番，旁邊的人不懂自然聽不出，但至少我短期內是達不到這樣的手速的。說是大俠，自然有大俠的風範，大俠有位同伴，大俠很高，很瘦，很冷，就是大俠的樣子。 上了火車也是翻譯，壹會弄完了，就開始玩遊戲啦，把遊戲從90%玩到了93%的同步率，也是累和淚，就是找不著那圈錄像帶，還有好幾個聖誕小人也不見了，怕是消失在存檔bug裏了TAT.（此處盜壹張遊戲截圖)（等等這個井蓋能打開？）\n到了杭州，\n下來就上了小黃車，騎行到了鼓樓，苦苦尋覓就是找不著青旅，來回奔波，問了店員，扯了好壹會才找著。店裏裝修的很是不錯，這個價格可以說很良心了，和國外的青旅類似，洗漱間與洗澡間廁所是男女共用的。我訂的女生六人間，裝修簡單溫馨，床板很高，裏面還有壹個小燈，拉上簾子就是六個人各自的私密小空間。不得不吐槽的是，這個床墊的套子實在是太小了，弄個20分鐘我估計也弄不上去，將就著睡還是可以的。 小小收拾壹下，包裏揣的滿滿的就出門了。出門左拐就是條美食街，牌坊上是這麽說，實則冷清，或許不是旺季，遊客太少。走著壹處小巷口，開著的綠皮鐵門裏面有個賣蔥包燴、銀耳湯的地方，還賣了或許是綠豆湯的東西。攤主是個小老頭，穿個軍大衣，旁邊和鄰居閑聊的怕是他的老婆。老頭的鍋黑乎乎的，手也臟臟的，看著不幹凈，但就是敢吃。脆餅包蔥，薄皮再裹脆餅，拿小鐵片壹壓，加上熱起鍋再加上料，或甜或辣，看個人口味。我要了個辣味的蔥包燴，帶著四川人的尊嚴還是不願點個甜的。說實話，也不是很好吃，收了我十個大洋，想想西湖高物價，我忍。\n走著走著人越來越少，燈也越來越暗。看著壹處牌指著城隍閣，可能是和上海的城隍廟是親戚，我搜壹下就拐到巷子裏去了。路上全是最平靜的質樸的百姓生活，洗頭、修車、賣水果，看不出熱鬧。配著黃黃的路燈，沒有物質繁榮蒸蒸日上的繁華，就是簡樸。\n我壹個人在路上，擺著各種姿勢拍照，手裏還拿著蔥包燴，路人倒也見怪不怪。這裏拍著壹張《共享單車之死》，名字是剛剛寫到這才取的，估計有點抄襲微博“低調的亮亮”之嫌，然而人水平可高了，我等實屬不入流。\n小巷越走越深，越走越暗，越走越往上，擡頭壹看，就是城隍閣 了。\n上山路上靜悄悄，我心裏就開始多想，想女鬼，想山怪，周圍人家還時不時傳來壹些聲響，或是鍋碗瓢盆、或者人聲弄得壹驚壹乍，更是心慌。走著走著，穿來壹聲狗叫，然後人希希爍爍的說話聲。嚇得我以為撞見了違法犯罪的場面。內心壹橫，不行，得上山。遠遠瞧見白狗壹條，二人圍著狗，不知在狗脖子那搗鼓什麽，狗還在掙紮，怕不是狗販子。仔細壹瞧，大叔還穿著睡衣，怕是出來散步，那狗也是薩摩耶，也不瞧我，看來是溫順的。順著路壹路走，左拐，上坡，右轉，直走。來到壹處寫著《吳山大觀》，好了，這是吳山。\n吳山上都是鍛煉的人，走在妳後面，小聲說著話，走路的鞋跟打的duangduang的，怪嚇人，像是在索命。得虧前面壹個鍛煉的大媽，我跟著她壹路下山，路上碰上壹對膩歪的情侶。男的說：“我背不動妳。要不把妳裙子撩起來。”兩人就開始笑，哼！現在想起來，前面的大媽若是女鬼，我不也是著了道，哈哈。 往下壹拐，路口上站壹個警察，旁邊豎壹個牌子：步行街，機動車不得入內。大概是這個意思，我憑著明銳的直覺，就奔著燈火通明去了。我看著路上人來人往，比美食街不是熱鬧到哪裏，當然東西都壹樣貴。手機壹查，果不其然，就是河坊街。有光的地方就有好照片。接下來就是壹波圖。\n我這人像鏡頭，不適合拍遠景，各位看看便知。\n河坊街主打復古懷舊，很多以老物件為賣點的店鋪小攤。五六十年代的紅軍軍裝照、上海灘民國照等等，都是景區的標配了。意外的是商店裏面特意買來裝飾的小物件:壹本《普適物理詞典》(名字記不大清)壹本《新漢語詞典》，都是真的老物件，幾毛錢，背後還有借書卡。上面有壹個寶麗來，下面壓壹個cd機。寶麗來壹按按鈕，還能彈出膠卷盒，得意壹般：別看我老，我還行。\n轉過頭就是電視收音機攝像機，如此壹來怕也是真貨，不知有什麽故事，錄下拍下過什麽人什麽快樂什麽淚，哎呀跑題了。\n這邊景區都是不要錢的，但看出來花了很多心思，似乎周圍也是美術文化底蘊豐厚的地方，處處都是設計、人文。\n路上買了塊發糕，6個十五塊，真是西湖本色，味道也是不合胃口的。（寫到現在真是累，但越寫越晚。再讓我想想）逛了河坊街，出了城門，居然又繞回了鼓樓。心壹想，我這傻得，明明這麽近，轉念又壹想，出來玩就是瘋，不好玩不自由被他人約束便無趣了。\n手機壹看地圖，河坊街走到底貌似就是湖邊，雖說手機快沒電，但還是又繞了回去。（流水賬也能這麽長）沒想到越繞越歪，只能打道回府，回了旅店，拿了充電寶，搜了地圖，直奔湖邊。對，還在鼓樓店下買了塊臭豆腐，10塊也是難吃，怎麽那麽多人會在那邊買吃的。\n按著心裏“河坊街到底是西湖邊”的想法，又繞進去昏了頭。無奈，只好跳出包圍圈，正好碰上有人在回收小黃車，手壹拜：“妳騎走吧，鎖上就行。”我便免費騎了壹次ofo。\n騎行了十分鐘來到“西湖天地”，此處多為酒吧，安靜的氛圍和慵懶的歌聲很讓人放松，若我非壹人前來，定要和朋友進去喝上幾盅。聽著歌聲，踏著輕步，看著周圍散步的男女，慢慢沿著小道向前走，也不知道這條路會通往何處，感覺就該向這邊走。仔細想來，整個旅程也充滿了隨意、任性、沖動，和近來的內心境況也是類似。\n到了西湖邊，已經11點。杭州不算冷，我又是騎車前來，身上特別暖和。四周沒什麽烈風，又特別安靜，有小的人聲，更多的聽見的是自己的聲音。沿湖坐在長椅上，兩邊垂著柳樹，看著對面的星點光斑，湖面的反光上下波動起伏，自己的思緒也隨著壹起。\n突然萌生出想要來看日出的想法，便不能多逗留，和朋友聊了聊，就起身回去。路上不乏在長椅上妳儂我儂的小情侶，甚是浪漫（汪汪！）。\n騎車回到旅店，想著翻譯還沒給同學發過去，便操起小電腦，在壹樓忙起來。此時壹樓人還很多，放著王尼瑪的被禁脫口秀，大家看的笑呵呵，有人在喝茶，有人喝前臺聊天，有人在擼貓，看不出什麽緊張。12點壹到，電視壹關，人也就散了，剩了我和需要熬夜的前臺，那個和前臺不停聊天、普通話蹩腳的福建人。簡單描述壹下，齊劉海、薄條紋毛衣，小眼突嘴，行為舉止有些娘炮。（妳還好意思說別人哈哈）（我這是客觀的描述，難不成要說行為舉止盡顯溫柔嗎）\n此處說明：前為2017-12旅行後立寫遊記，但僅寫了壹晚，[後續]({% post_url 2017-12-01-tour2 %})——為此次補充，時間2018-1-30,記憶不清，見諒\n","permalink":"https://Vacationwkx.github.io/posts/tour/","summary":"游记 下\n 今日正經地寫壹篇遊記，紀念這次突然起意卻又收獲頗豐的匆匆旅行。\n先列舉壹些第壹次：\n 第壹次壹個人出遊； 第壹次在火車站“裝逼”用電腦，但是真正的用了，翻譯了文檔，效率奇高； 第壹次在青旅還在小小學習了壹下，我承認，這是真裝逼； 杭州大叔大伯都迅速準確地稱呼了我，小姑娘難得，倒是壹個外國人尊敬地說了“Excuse me，sir” …  再說壹些原來，這個純粹是我見識淺薄，大家不要見笑：\n 原來白素貞是仙女下凡，並非僅是妖怪，被徐克老怪坑害不淺； 原來法海才是妖怪，為王八精； 原來斷橋不是斷的，路鋪的好得很，都能跑車了； 原來蘇堤和白堤是蘇軾和白居易的堤，相比蘇堤的安恬，白堤意外的熱鬧，詳見後敘； 原來杭州美女也不溫婉，是特別社會、時尚的，現代化的女性更多； …  現在，讓我們開始倒帶回憶： 上完課，本想和老師討論討論，於3:08緊急接到師傅通知，讓我立馬出發。似乎開了壹個壞頭，整個旅程也是匆匆而行。\n到站後，便開始了電腦“裝逼”，只見我嫻熟地掏出電腦，嫻熟地用指紋開了電腦(看看這嘴臉，現在還得意呢)。然後開始了頭痛的論文翻譯校對，這壹弄，頭也昏了，腰也酸了，得虧吃了麥當勞緩了過來（這下不得不提杭州站的肯德基，效率低，服務人員也少，簡直不像省會）。候車的座位旁邊壹家商店恰好有壹套小的便攜電子折疊鼓，先後有好幾個人來打了，都是胡亂打，我這個破水平都聽得出來，店員稍微好壹點，有點節奏感。不過，後來來了壹位大俠，神乎其技地演奏了壹番，旁邊的人不懂自然聽不出，但至少我短期內是達不到這樣的手速的。說是大俠，自然有大俠的風範，大俠有位同伴，大俠很高，很瘦，很冷，就是大俠的樣子。 上了火車也是翻譯，壹會弄完了，就開始玩遊戲啦，把遊戲從90%玩到了93%的同步率，也是累和淚，就是找不著那圈錄像帶，還有好幾個聖誕小人也不見了，怕是消失在存檔bug裏了TAT.（此處盜壹張遊戲截圖)（等等這個井蓋能打開？）\n到了杭州，\n下來就上了小黃車，騎行到了鼓樓，苦苦尋覓就是找不著青旅，來回奔波，問了店員，扯了好壹會才找著。店裏裝修的很是不錯，這個價格可以說很良心了，和國外的青旅類似，洗漱間與洗澡間廁所是男女共用的。我訂的女生六人間，裝修簡單溫馨，床板很高，裏面還有壹個小燈，拉上簾子就是六個人各自的私密小空間。不得不吐槽的是，這個床墊的套子實在是太小了，弄個20分鐘我估計也弄不上去，將就著睡還是可以的。 小小收拾壹下，包裏揣的滿滿的就出門了。出門左拐就是條美食街，牌坊上是這麽說，實則冷清，或許不是旺季，遊客太少。走著壹處小巷口，開著的綠皮鐵門裏面有個賣蔥包燴、銀耳湯的地方，還賣了或許是綠豆湯的東西。攤主是個小老頭，穿個軍大衣，旁邊和鄰居閑聊的怕是他的老婆。老頭的鍋黑乎乎的，手也臟臟的，看著不幹凈，但就是敢吃。脆餅包蔥，薄皮再裹脆餅，拿小鐵片壹壓，加上熱起鍋再加上料，或甜或辣，看個人口味。我要了個辣味的蔥包燴，帶著四川人的尊嚴還是不願點個甜的。說實話，也不是很好吃，收了我十個大洋，想想西湖高物價，我忍。\n走著走著人越來越少，燈也越來越暗。看著壹處牌指著城隍閣，可能是和上海的城隍廟是親戚，我搜壹下就拐到巷子裏去了。路上全是最平靜的質樸的百姓生活，洗頭、修車、賣水果，看不出熱鬧。配著黃黃的路燈，沒有物質繁榮蒸蒸日上的繁華，就是簡樸。\n我壹個人在路上，擺著各種姿勢拍照，手裏還拿著蔥包燴，路人倒也見怪不怪。這裏拍著壹張《共享單車之死》，名字是剛剛寫到這才取的，估計有點抄襲微博“低調的亮亮”之嫌，然而人水平可高了，我等實屬不入流。\n小巷越走越深，越走越暗，越走越往上，擡頭壹看，就是城隍閣 了。\n上山路上靜悄悄，我心裏就開始多想，想女鬼，想山怪，周圍人家還時不時傳來壹些聲響，或是鍋碗瓢盆、或者人聲弄得壹驚壹乍，更是心慌。走著走著，穿來壹聲狗叫，然後人希希爍爍的說話聲。嚇得我以為撞見了違法犯罪的場面。內心壹橫，不行，得上山。遠遠瞧見白狗壹條，二人圍著狗，不知在狗脖子那搗鼓什麽，狗還在掙紮，怕不是狗販子。仔細壹瞧，大叔還穿著睡衣，怕是出來散步，那狗也是薩摩耶，也不瞧我，看來是溫順的。順著路壹路走，左拐，上坡，右轉，直走。來到壹處寫著《吳山大觀》，好了，這是吳山。\n吳山上都是鍛煉的人，走在妳後面，小聲說著話，走路的鞋跟打的duangduang的，怪嚇人，像是在索命。得虧前面壹個鍛煉的大媽，我跟著她壹路下山，路上碰上壹對膩歪的情侶。男的說：“我背不動妳。要不把妳裙子撩起來。”兩人就開始笑，哼！現在想起來，前面的大媽若是女鬼，我不也是著了道，哈哈。 往下壹拐，路口上站壹個警察，旁邊豎壹個牌子：步行街，機動車不得入內。大概是這個意思，我憑著明銳的直覺，就奔著燈火通明去了。我看著路上人來人往，比美食街不是熱鬧到哪裏，當然東西都壹樣貴。手機壹查，果不其然，就是河坊街。有光的地方就有好照片。接下來就是壹波圖。\n我這人像鏡頭，不適合拍遠景，各位看看便知。\n河坊街主打復古懷舊，很多以老物件為賣點的店鋪小攤。五六十年代的紅軍軍裝照、上海灘民國照等等，都是景區的標配了。意外的是商店裏面特意買來裝飾的小物件:壹本《普適物理詞典》(名字記不大清)壹本《新漢語詞典》，都是真的老物件，幾毛錢，背後還有借書卡。上面有壹個寶麗來，下面壓壹個cd機。寶麗來壹按按鈕，還能彈出膠卷盒，得意壹般：別看我老，我還行。\n轉過頭就是電視收音機攝像機，如此壹來怕也是真貨，不知有什麽故事，錄下拍下過什麽人什麽快樂什麽淚，哎呀跑題了。\n這邊景區都是不要錢的，但看出來花了很多心思，似乎周圍也是美術文化底蘊豐厚的地方，處處都是設計、人文。\n路上買了塊發糕，6個十五塊，真是西湖本色，味道也是不合胃口的。（寫到現在真是累，但越寫越晚。再讓我想想）逛了河坊街，出了城門，居然又繞回了鼓樓。心壹想，我這傻得，明明這麽近，轉念又壹想，出來玩就是瘋，不好玩不自由被他人約束便無趣了。\n手機壹看地圖，河坊街走到底貌似就是湖邊，雖說手機快沒電，但還是又繞了回去。（流水賬也能這麽長）沒想到越繞越歪，只能打道回府，回了旅店，拿了充電寶，搜了地圖，直奔湖邊。對，還在鼓樓店下買了塊臭豆腐，10塊也是難吃，怎麽那麽多人會在那邊買吃的。\n按著心裏“河坊街到底是西湖邊”的想法，又繞進去昏了頭。無奈，只好跳出包圍圈，正好碰上有人在回收小黃車，手壹拜：“妳騎走吧，鎖上就行。”我便免費騎了壹次ofo。\n騎行了十分鐘來到“西湖天地”，此處多為酒吧，安靜的氛圍和慵懶的歌聲很讓人放松，若我非壹人前來，定要和朋友進去喝上幾盅。聽著歌聲，踏著輕步，看著周圍散步的男女，慢慢沿著小道向前走，也不知道這條路會通往何處，感覺就該向這邊走。仔細想來，整個旅程也充滿了隨意、任性、沖動，和近來的內心境況也是類似。\n到了西湖邊，已經11點。杭州不算冷，我又是騎車前來，身上特別暖和。四周沒什麽烈風，又特別安靜，有小的人聲，更多的聽見的是自己的聲音。沿湖坐在長椅上，兩邊垂著柳樹，看著對面的星點光斑，湖面的反光上下波動起伏，自己的思緒也隨著壹起。\n突然萌生出想要來看日出的想法，便不能多逗留，和朋友聊了聊，就起身回去。路上不乏在長椅上妳儂我儂的小情侶，甚是浪漫（汪汪！）。\n騎車回到旅店，想著翻譯還沒給同學發過去，便操起小電腦，在壹樓忙起來。此時壹樓人還很多，放著王尼瑪的被禁脫口秀，大家看的笑呵呵，有人在喝茶，有人喝前臺聊天，有人在擼貓，看不出什麽緊張。12點壹到，電視壹關，人也就散了，剩了我和需要熬夜的前臺，那個和前臺不停聊天、普通話蹩腳的福建人。簡單描述壹下，齊劉海、薄條紋毛衣，小眼突嘴，行為舉止有些娘炮。（妳還好意思說別人哈哈）（我這是客觀的描述，難不成要說行為舉止盡顯溫柔嗎）\n此處說明：前為2017-12旅行後立寫遊記，但僅寫了壹晚，[後續]({% post_url 2017-12-01-tour2 %})——為此次補充，時間2018-1-30,記憶不清，見諒","title":"2017-12-01(1)-Not a serious Diary"},{"content":"Wir Alle Sind Naiv 2017.11.26\nAm 3. November 2016 war Jiangge, die 24 Jahre alt war und derzeit in Japan studierte, von Chen Shifeng umgebracht worden (ermordet). Die offiziele Autopsie von Jiangge behaupte, dass sie 19 mals vom Mörder zugestoßen habe, und die Wunden 10 cm seien. Der Verdächtiger Chen Shifeng ist der Ex-Freund von Jiangges Mitwohnerin Liu Xing. Liu scheint, über den Tod ihrer besten Freundin etwas Unechtes(tatsächlich/wirklich/wahr/waschecht) zu erklären, um eigenen Pflicht zu vermeiden, das auch den Ärger der Netizen erzeugt hat (auslösen/erregen/erwecken/wachrufen). Über den granzen Prozess kann man im Internet genau und deutlich sehen, dann würde ich dazu im diesem Artikel niemehr artikuliere (aussprechen).\nWas ich aus tiefstem Herzen aussprechen möchte, ist was wir jetzt machen können. Vor allem muss der wahre Mörder legal ahndet werden (bestrafen). Gagegen konzentrieren sich die Internetnutzer ganz auf die Herzlosigkeit, Unrichtigkeit und Ungerechtigkeit (unfair) der Liu Xing. Ich denke, dass Mensch kein Gott ist, dann es gehört nicht zu unserem Recht, irgendwen öffentlich zu beurteilen. Viele vergessen, das einfache Urteil hat die schweren Auswirkungen in 1966-1976. Wer kann sich versprechen, dass er unbedingt die Tür für Jiangge öffnen würde, when Chen Shifeng ein Messer in der Hand hielt, und Jiangge schon von Chen zugestoßen hätte, und wahrscheinlich sie auch vielmals geschrieen hätte. Hat ein Mensch trotz seiner instinktartigen Reaktion (Instinkt) vor Gefahr (gefährlich) gegen einen mehrmals stärker als sich Mörder und die große Angst (ängstlich) das machen, wenn es auch eine Möglichkeit gibt, ums Leben zu kommen.\nJiangge und Liu Xing sind die Menschen in friedlich Zeit geboren wie andere 20- und 30-jährige junge Leute, deswegen haben sie keine Erfahrungen über den Ernstfall (Notfall), außerdem ist die Hochherzigkeit, die immer in Kriegsjahre mehrmals von der Regierung genant, und die auch uns unsere Eltern in der Kindheit vermittelt haben, heutzutage nicht üblich mehr. Heute ist das Geld wichtigst und bedeutendest, weil die Regierung dadurch direkt Profite in privaten Geldbeutel besorgen und seine Macht immer nehmen kann.\nIch verstehe, wieso die Leute über das Verhalten der Liu Xing ängerlich sind. Liu Xings „beste“ Freundin ist nur außer einer Tür und knapp tot, weil sie ihre Mitwohnerin vor einem gefährlichen Mann schützen möchte. Ein Leben um ein anderes Leben zu tauschen, scheint nichts für die „Überlebende“. Alle sind möglich der Meinung, dass sie schon erwachsen sind, und sie die Verantwortung für eigenem tragen müssen, aber sie sind nur reine Mädchen vor einem 80-jährigen Leben und der komplizierten Gesellschaft. Natürlich erwarten man die Schönheit, dass jeder in besonderer Situation einen klaren Gedankengang hält und sich genau benimmt, was in Tatsache kaum entstehen kann.\nLetzlich würde ich sagen, was wir sind, ist nicht nur was wir weisen, auch was wir tun. Chinesicher Sprichtwort lautet auch: „ Sagen ist leichter als machen.“\nDie Gerechtigkeit von Jiangge ist nicht futsch (verloren/verschwunden), irgendwann wird Chen Shifeng bestraftet. Ich bedanke mich bei dieser mutigen Mädchen für ihre Betrag, um ein anderes Leben zu retten. Sie ist edel, was ist entschließend.\n(PS:Der Sinn eines Lebens ist anders als der eines Mensches, hier will ich dazu nichts erklären, darüber kann man im nächsten Artikel sehen.)\n","permalink":"https://Vacationwkx.github.io/posts/csf/","summary":"Wir Alle Sind Naiv 2017.11.26\nAm 3. November 2016 war Jiangge, die 24 Jahre alt war und derzeit in Japan studierte, von Chen Shifeng umgebracht worden (ermordet). Die offiziele Autopsie von Jiangge behaupte, dass sie 19 mals vom Mörder zugestoßen habe, und die Wunden 10 cm seien. Der Verdächtiger Chen Shifeng ist der Ex-Freund von Jiangges Mitwohnerin Liu Xing. Liu scheint, über den Tod ihrer besten Freundin etwas Unechtes(tatsächlich/wirklich/wahr/waschecht) zu erklären, um eigenen Pflicht zu vermeiden, das auch den Ärger der Netizen erzeugt hat (auslösen/erregen/erwecken/wachrufen).","title":"Der Tod eines Mannen - ChenShifeng"},{"content":"Idol Xue Was ist die Pflicht des Sängers? Natürlich, Singen. Aber im letzten Tage ist ein komisches Ereignis passiert. Der Sänger in China, Xue Zhiqian, heiratete mit seiner Ex-Frau, was ein vollkommenes Ende sein soll. Aber er hat danach zwei Tadel bekommen, die Koinzidenz zu sein scheinen. Diese drei Bomben herrscht Weibo, damit alle sich daüber unterhalten, sondern über seine Werke.\nWhat is the duty of a singer? Simply, sing. But in the past few days a strange thing happened. The singer in China, Xue Zhiqian, married with his ex-wife, which should be a happy ending. However, he was rebuked twice. This might sound like a coincidence. These three booms exploded the Weibo, and almost everyone was talking about these, rather than the works of the singer.\nEine Ursache der Farce ist vermutlich, dass er in einem Live-show über die offiziellen Gruppe geschimpft hat, weil sie ihm befahlen, um einen virtuelle Kandidat zu unterstützen, obwohl ein anderer Spieler ihm gefallen hat. Tatsächlich ist der Kandidat von einem Unternehmen, QQ video, geschaffen worden, das einen Zweig des QQ Reiches ist. Derzeit war der Wettkampf zwischen den zwei ,,Personen‘‘, und der Spieler hat schon das Oberwasser. Trotzdem hat seine Wahl es zu Folge, dass der Spieler ausgeschieden hat, was aus der Erwartung des Sängers war. Dann er hat das Gefühl, dass er weisgemacht wurde.\nOne of the reasons of the farce maybe that, that he censured a official group on his Live-show. They seemed order him to support a virtual candidate, even though he actually thought the orther one is better. The truth is , the virtual candidate belongs to QQ video, a subordinary company of QQ empire. At that time these two persons(actually a man and a computer) had a fight, and the man had a obvious advantage. On the contrary, the singer’s choice led to the fail of the man, which beyonded Xue’s estimation. So he had a feeling, that he had been dumped on.\nIch bin der Meinung, dass er unbedingt das Kommando nie annehmen soll, weil er ein rechtschaftener Mann ist.\nI just think, the singer should no matter what the order reject, because he is such a fair man.\n","permalink":"https://Vacationwkx.github.io/posts/saenger/","summary":"Idol Xue Was ist die Pflicht des Sängers? Natürlich, Singen. Aber im letzten Tage ist ein komisches Ereignis passiert. Der Sänger in China, Xue Zhiqian, heiratete mit seiner Ex-Frau, was ein vollkommenes Ende sein soll. Aber er hat danach zwei Tadel bekommen, die Koinzidenz zu sein scheinen. Diese drei Bomben herrscht Weibo, damit alle sich daüber unterhalten, sondern über seine Werke.\nWhat is the duty of a singer? Simply, sing. But in the past few days a strange thing happened.","title":"Der Saenger"},{"content":"","permalink":"https://Vacationwkx.github.io/tags/","summary":"tags","title":"Tags"}]