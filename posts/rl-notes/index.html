<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reinforcement Learning Notes | 無極</title><meta name=keywords content="reinforcement learning"><meta name=description content="State Representation  state vector ( obey Markov Property) observation →knowledge→ state Partially observable Markov decision process &ldquo;learning or classification algorithms to &ldquo;learn&rdquo; those states&rdquo;  A simple linear regression A more complex non-linear function approximator, such as a multi-layer neural network.    The Atari DQN work by DeepMind team used a combination of feature engineering and relying on deep neural network to achieve its results. The feature engineering included downsampling the image, reducing it to grey-scale and - importantly for the Markov Property - using four consecutive frames to represent a single state, so that information about velocity of objects was present in the state representation."><meta name=author content="Vac"><link rel=canonical href=https://Vacationwkx.github.io/posts/rl-notes/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.04512c372388e08b5118f5b117b2d3efef4ddae52017e16085c8d8d4e361c43d.css integrity="sha256-BFEsNyOI4ItRGPWxF7LT7+9N2uUgF+FghcjY1ONhxD0=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://Vacationwkx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://Vacationwkx.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Vacationwkx.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Vacationwkx.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://Vacationwkx.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.83.0"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="Reinforcement Learning Notes"><meta property="og:description" content="State Representation  state vector ( obey Markov Property) observation →knowledge→ state Partially observable Markov decision process &ldquo;learning or classification algorithms to &ldquo;learn&rdquo; those states&rdquo;  A simple linear regression A more complex non-linear function approximator, such as a multi-layer neural network.    The Atari DQN work by DeepMind team used a combination of feature engineering and relying on deep neural network to achieve its results. The feature engineering included downsampling the image, reducing it to grey-scale and - importantly for the Markov Property - using four consecutive frames to represent a single state, so that information about velocity of objects was present in the state representation."><meta property="og:type" content="article"><meta property="og:url" content="https://Vacationwkx.github.io/posts/rl-notes/"><meta property="og:image" content="https://Vacationwkx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-12-08T08:47:38+08:00"><meta property="article:modified_time" content="2020-12-08T08:47:38+08:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Vacationwkx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Reinforcement Learning Notes"><meta name=twitter:description content="State Representation  state vector ( obey Markov Property) observation →knowledge→ state Partially observable Markov decision process &ldquo;learning or classification algorithms to &ldquo;learn&rdquo; those states&rdquo;  A simple linear regression A more complex non-linear function approximator, such as a multi-layer neural network.    The Atari DQN work by DeepMind team used a combination of feature engineering and relying on deep neural network to achieve its results. The feature engineering included downsampling the image, reducing it to grey-scale and - importantly for the Markov Property - using four consecutive frames to represent a single state, so that information about velocity of objects was present in the state representation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://Vacationwkx.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Reinforcement Learning Notes","item":"https://Vacationwkx.github.io/posts/rl-notes/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reinforcement Learning Notes","name":"Reinforcement Learning Notes","description":"State Representation  state vector ( obey Markov Property) observation →knowledge→ state Partially observable Markov decision process \u0026ldquo;learning or classification algorithms to \u0026ldquo;learn\u0026rdquo; those states\u0026rdquo;  A simple linear regression A more complex non-linear function approximator, such as a multi-layer neural network.    The Atari DQN work by DeepMind team used a combination of feature engineering and relying on deep neural network to achieve its results. The feature engineering included downsampling the image, reducing it to grey-scale and - importantly for the Markov Property - using four consecutive frames to represent a single state, so that information about velocity of objects was present in the state representation.","keywords":["reinforcement learning"],"articleBody":"State Representation  state vector ( obey Markov Property) observation →knowledge→ state Partially observable Markov decision process “learning or classification algorithms to “learn” those states”  A simple linear regression A more complex non-linear function approximator, such as a multi-layer neural network.    The Atari DQN work by DeepMind team used a combination of feature engineering and relying on deep neural network to achieve its results. The feature engineering included downsampling the image, reducing it to grey-scale and - importantly for the Markov Property - using four consecutive frames to represent a single state, so that information about velocity of objects was present in the state representation. The DNN then processed the images into higher-level features that could be used to make predictions about state values.\nPolicy Gradients Deep Deterministic Policy Gradient - Spinning Up documentation\nMathe Funda\nDeep Reinforcement Learning VDQN DeepMind used atari environment for DQN test, even through all the return observation for preprocessing:\nPreprocessing  Observation  rgb $\\rightarrow$ gray, i.e. image shape (210,160,3)$\\rightarrow$ (210,160) down sample: (210,160) $\\rightarrow$ (110,84) crop: (110,84) $\\rightarrow$ (84,84)   Observation :arrow_right: state:  4 history frames to 1 $\\rightarrow$ (84,84,4)    CNN   architecture\n Conv2D: 32, 8x8, strides=4, input_shape=(84,84,4) Conv2D: 64, 4x4, strides=2 Conv2D: 64, 3x3, strides=1 Dense: 256 Dens: outputshape    comple\n RMSProp    Replay Buffer  fix length every time feed: (state, action, reward, next_state, done) once reach L length: start training length: 1million  Target model update  every C step Epsilon: 1$\\rightarrow$0.1 (1 million frame for total 50 milion)  Frame skipping great explanation\n chose at kth frame last k-1 frame k=4  Speed up for atari, use info['ale.lives'] for terminating the episode\nClip  reward: -1,0,1 Error: $|Q(s,a,\\theta)-Q(s',a',\\theta^-)\\le1$  NOTES:\nchange RMSprop parameter\ntf.keras.optimizers.RMSprop( learning_rate=0.00025, rho=0.9, momentum=0.95, epsilon=1e-07, centered=False, name=\"RMSprop\", **kwargs ) Random Gym Run  Frame:10,000 Reward: 4-2, 5-0  Double DQN  main: choose action target: update Q in Bellman as Q_max model fit: fit the main with target value output after same iterations, copy main weights and biases to target  DuelingDQN maybe drop DQN\nsource\n DDQN Architecture custom model fit for weights too complicated Priorited Replay  paper code test   Agent Train Main  DQN Parameter Adjustment https://github.com/dennybritz/reinforcement-learning/issues/30\nhttps://www.reddit.com/r/reinforcementlearning/comments/7kwcb5/need_help_how_to_debug_deep_rl_algorithms/\nA3C  David Sliver Policy Gradient Mathe Paper review Example threading Pytorch Code for Pendulum  A2C Why A2C not A3C\n Two head Network Why Entropy: https://awjuliani.medium.com/maximum-entropy-policies-in-reinforcement-learning-everyday-life-f5a1cc18d32d#:~:text=Because%20RL%20is%20all%20about,the%20actions%20an%20agent%20takes.\u0026text=In%20RL%2C%20the%20goal%20is,term%20sum%20of%20discounted%20rewards  Entropy is great, but you might be wondering what that has to do with reinforcement learning and this A2C algorithm we discussed. The idea here is to use entropy to encourage further exploration of the model to prevent premature convergence   Negative Loss:  TensorFlow and PyTorch currently don’t have the ability to maximize a function, we then minimize the negative of our loss   Accumulated gradient  PPO   debug for sub optimal:\n  possible solutions\n decrease lr decrease lambda during program    It would be helpful to output more metrics, such as losses, norms of the gradients, KL divergence between your old and new policies after a number of PPO updates.source\n  change algo\n It depends on the environment. Something like ball balancing might just tend to destabilize with PPO, vs. something like half cheetah that is less finicky balance wise. You might try using td3 or sac, but with ppo you might just have to early stop. There might be some perfect combo of lr and clip param that leaves it stabilized… maybe with using another optimizer as well like classic momentum or adagrad.\nsource\n      Material Powerup Knowledge  How to Exploration Why RL Hard DRL Sucks  Intro An Introduction to Deep Reinforcement Learning\nDRL Reward shaping reference\nCode for model  Tensorflow Pytorch  Simple to create Model PPO    Course   CS 285\n  Deep Reinforcement Learning\n  Blog  Atari Medium Tensorflow Code Start from PONG Nice Dude  Atari  Monitor / Video using X11 Recap  CNN  parameter calculation : https://medium.com/@iamvarman/how-to-calculate-the-number-of-parameters-in-the-cnn-5bd55364d7ca#:~:text=To%20calculate%20it%2C%20we%20have,3%E2%80%931))%20%3D%2048 output shape calculation  API for Vrep Legacy remote API\nRemote API functions (Python)\nDavid Silver - 4/10 Teaching - David Silver\nThe Book - Ch5 Reinforcement Learning: An Introduction\nCode for it:\nReinforcement Learning: An Introduction\nPaper Keypaper\nTraining Software Gym: A toolkit for developing and comparing reinforcement learning algorithms\nOpen cv  resize  ","wordCount":"664","inLanguage":"en","datePublished":"2020-12-08T08:47:38+08:00","dateModified":"2020-12-08T08:47:38+08:00","author":{"@type":"Person","name":"Vac"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Vacationwkx.github.io/posts/rl-notes/"},"publisher":{"@type":"Organization","name":"無極","logo":{"@type":"ImageObject","url":"https://Vacationwkx.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://Vacationwkx.github.io accesskey=h title="Home (Alt + H)"><img src=/apple-touch-icon.png alt=logo aria-label=logo height=35>Home</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://Vacationwkx.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://Vacationwkx.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://Vacationwkx.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://Vacationwkx.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Vacationwkx.github.io>Home</a>&nbsp;»&nbsp;<a href=https://Vacationwkx.github.io/posts/>Posts</a></div><h1 class=post-title>Reinforcement Learning Notes</h1><div class=post-meta>December 8, 2020&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Vac</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#state-representationhttpsaistackexchangecomquestions7763how-to-define-states-in-reinforcement-learning aria-label="State Representation"><a href=https://ai.stackexchange.com/questions/7763/how-to-define-states-in-reinforcement-learning>State Representation</a></a></li><li><a href=#policy-gradients aria-label="Policy Gradients">Policy Gradients</a></li><li><a href=#deep-reinforcement-learning aria-label="Deep Reinforcement Learning">Deep Reinforcement Learning</a><ul><li><a href=#vdqn aria-label=VDQN>VDQN</a><ul><li><a href=#preprocessing aria-label=Preprocessing>Preprocessing</a></li><li><a href=#cnn aria-label=CNN>CNN</a></li><li><a href=#replay-buffer aria-label="Replay Buffer">Replay Buffer</a></li><li><a href=#target-model-update aria-label="Target model update">Target model update</a></li><li><a href=#frame-skipping aria-label="Frame skipping">Frame skipping</a></li><li><a href=#clip aria-label=Clip>Clip</a></li><li><a href=#random-gym-run aria-label="Random Gym Run">Random Gym Run</a></li></ul></li><li><a href=#double-dqnhttpsmediumcomanalytics-vidhyabuilding-a-powerful-dqn-in-tensorflow-2-0-explanation-tutorial-d48ea8f3177a aria-label="Double DQN"><a href=https://medium.com/analytics-vidhya/building-a-powerful-dqn-in-tensorflow-2-0-explanation-tutorial-d48ea8f3177a>Double DQN</a></a></li><li><a href=#duelingdqnhttpstowardsdatasciencecomdueling-deep-q-networks-81ffab672751 aria-label=DuelingDQN><a href=https://towardsdatascience.com/dueling-deep-q-networks-81ffab672751>DuelingDQN</a></a></li><li><a href=#dqn-parameter-adjustment aria-label="DQN Parameter Adjustment">DQN Parameter Adjustment</a></li><li><a href=#a3c aria-label=A3C>A3C</a></li><li><a href=#a2c aria-label=A2C>A2C</a></li><li><a href=#ppo aria-label=PPO>PPO</a></li></ul></li><li><a href=#material aria-label=Material>Material</a><ul><ul><li><a href=#powerup-knowledge aria-label="Powerup Knowledge">Powerup Knowledge</a></li></ul><li><a href=#intro aria-label=Intro>Intro</a></li><li><a href=#drl aria-label=DRL>DRL</a><ul><li><a href=#reward-shaping aria-label="Reward shaping">Reward shaping</a></li><li><a href=#code-for-model aria-label="Code for model">Code for model</a></li><li><a href=#course aria-label=Course>Course</a></li><li><a href=#blog aria-label=Blog>Blog</a></li><li><a href=#atari aria-label=Atari>Atari</a></li><li><a href=#cnn-1 aria-label=CNN>CNN</a></li></ul></li><li><a href=#api-for-vrep aria-label="API for Vrep">API for Vrep</a></li><li><a href=#david-silver---410 aria-label="David Silver - 4/10">David Silver - 4/10</a></li><li><a href=#the-book---ch5 aria-label="The Book - Ch5">The Book - Ch5</a></li><li><a href=#paper aria-label=Paper>Paper</a></li><li><a href=#training-software aria-label="Training Software">Training Software</a></li><li><a href=#open-cv aria-label="Open cv">Open cv</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=state-representationhttpsaistackexchangecomquestions7763how-to-define-states-in-reinforcement-learning><a href=https://ai.stackexchange.com/questions/7763/how-to-define-states-in-reinforcement-learning>State Representation</a><a hidden class=anchor aria-hidden=true href=#state-representationhttpsaistackexchangecomquestions7763how-to-define-states-in-reinforcement-learning>#</a></h2><ul><li>state vector ( obey Markov Property)</li><li>observation →knowledge→ state</li><li><a href=https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process>Partially observable Markov decision process</a></li><li>&ldquo;learning or classification algorithms to &ldquo;learn&rdquo; those states&rdquo;<ul><li>A simple linear regression</li><li>A more complex non-linear function approximator, such as a multi-layer neural network.</li></ul></li></ul><p>The Atari DQN work by DeepMind team used a combination of feature engineering and relying on deep neural network to achieve its results. The feature engineering included downsampling the image, reducing it to grey-scale and - importantly for the Markov Property - using four consecutive frames to represent a single state, so that information about velocity of objects was present in the state representation. The DNN then processed the images into higher-level features that could be used to make predictions about state values.</p><h2 id=policy-gradients>Policy Gradients<a hidden class=anchor aria-hidden=true href=#policy-gradients>#</a></h2><p><a href=https://spinningup.openai.com/en/latest/algorithms/ddpg.html>Deep Deterministic Policy Gradient - Spinning Up documentation</a></p><p><a href=https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/>Mathe Funda</a></p><h2 id=deep-reinforcement-learning>Deep Reinforcement Learning<a hidden class=anchor aria-hidden=true href=#deep-reinforcement-learning>#</a></h2><h3 id=vdqn>VDQN<a hidden class=anchor aria-hidden=true href=#vdqn>#</a></h3><p><img loading=lazy src=/rl/dqn.png alt=DQN></p><p>DeepMind used atari environment for DQN test, even through all the return <code>observation</code> for preprocessing:</p><h4 id=preprocessing>Preprocessing<a hidden class=anchor aria-hidden=true href=#preprocessing>#</a></h4><ul><li>Observation<ol><li>rgb $\rightarrow$ gray, i.e. image shape (210,160,3)$\rightarrow$ (210,160)</li><li>down sample: (210,160) $\rightarrow$ (110,84)</li><li>crop: (110,84) $\rightarrow$ (84,84)</li></ol></li><li>Observation :arrow_right: state:<ol><li>4 history frames to 1 $\rightarrow$ (84,84,4)</li></ol></li></ul><h4 id=cnn>CNN<a hidden class=anchor aria-hidden=true href=#cnn>#</a></h4><p><img loading=lazy src=/rl/atari_cnn.png alt="CNN of DQN"></p><ul><li><p>architecture</p><ul><li>Conv2D: 32, 8x8, strides=4, input_shape=(84,84,4)</li><li>Conv2D: 64, 4x4, strides=2</li><li>Conv2D: 64, 3x3, strides=1</li><li>Dense: 256</li><li>Dens: outputshape</li></ul></li><li><p>comple</p><ul><li>RMSProp</li></ul></li></ul><h4 id=replay-buffer>Replay Buffer<a hidden class=anchor aria-hidden=true href=#replay-buffer>#</a></h4><ul><li>fix length</li><li>every time feed: (state, action, reward, next_state, done)</li><li>once reach L length: start training</li><li>length: 1million</li></ul><h4 id=target-model-update>Target model update<a hidden class=anchor aria-hidden=true href=#target-model-update>#</a></h4><ul><li>every C step</li><li>Epsilon: 1$\rightarrow$0.1 (1 million frame for total 50 milion)</li></ul><h4 id=frame-skipping>Frame skipping<a hidden class=anchor aria-hidden=true href=#frame-skipping>#</a></h4><p><img loading=lazy src=/rl/frame_skip.png alt="Frame skip"></p><p><a href=https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/>great explanation</a></p><ul><li>chose at kth frame</li><li>last k-1 frame</li><li>k=4</li></ul><p>Speed up for atari, use <code>info['ale.lives'] &lt; 5</code> for terminating the episode</p><h4 id=clip>Clip<a hidden class=anchor aria-hidden=true href=#clip>#</a></h4><ul><li>reward: -1,0,1</li><li>Error: $|Q(s,a,\theta)-Q(s',a',\theta^-)\le1$</li></ul><p>NOTES:</p><p>change RMSprop parameter</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>optimizers<span style=color:#f92672>.</span>RMSprop(
    learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.00025</span>,
    rho<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>,
    momentum<span style=color:#f92672>=</span><span style=color:#ae81ff>0.95</span>,
    epsilon<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-07</span>,
    centered<span style=color:#f92672>=</span>False,
    name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;RMSprop&#34;</span>,
    <span style=color:#f92672>**</span>kwargs
)
</code></pre></div><h4 id=random-gym-run>Random Gym Run<a hidden class=anchor aria-hidden=true href=#random-gym-run>#</a></h4><ul><li>Frame:10,000</li><li>Reward: 4-2, 5-0</li></ul><h3 id=double-dqnhttpsmediumcomanalytics-vidhyabuilding-a-powerful-dqn-in-tensorflow-2-0-explanation-tutorial-d48ea8f3177a><a href=https://medium.com/analytics-vidhya/building-a-powerful-dqn-in-tensorflow-2-0-explanation-tutorial-d48ea8f3177a>Double DQN</a><a hidden class=anchor aria-hidden=true href=#double-dqnhttpsmediumcomanalytics-vidhyabuilding-a-powerful-dqn-in-tensorflow-2-0-explanation-tutorial-d48ea8f3177a>#</a></h3><ul><li>main: choose action</li><li>target: update Q in Bellman as Q_max</li><li>model fit: fit the main with target value output</li><li>after same iterations, copy main weights and biases to target</li></ul><h3 id=duelingdqnhttpstowardsdatasciencecomdueling-deep-q-networks-81ffab672751><a href=https://towardsdatascience.com/dueling-deep-q-networks-81ffab672751>DuelingDQN</a><a hidden class=anchor aria-hidden=true href=#duelingdqnhttpstowardsdatasciencecomdueling-deep-q-networks-81ffab672751>#</a></h3><p>maybe drop DQN</p><p><img loading=lazy src=/rl/drl_compare.png alt="Perform on atari"></p><p><a href=https://www.toptal.com/machine-learning/deep-dive-into-reinforcement-learning>source</a></p><ul><li><input checked disabled type=checkbox> DDQN Architecture</li><li><input checked disabled type=checkbox> <del>custom model fit for weights</del> too complicated</li><li><input checked disabled type=checkbox> Priorited Replay<ul><li><input checked disabled type=checkbox> paper</li><li><input checked disabled type=checkbox> code</li><li><input checked disabled type=checkbox> test</li></ul></li><li><input checked disabled type=checkbox> Agent</li><li><input checked disabled type=checkbox> Train Main</li></ul><h3 id=dqn-parameter-adjustment>DQN Parameter Adjustment<a hidden class=anchor aria-hidden=true href=#dqn-parameter-adjustment>#</a></h3><p><a href=https://github.com/dennybritz/reinforcement-learning/issues/30>https://github.com/dennybritz/reinforcement-learning/issues/30</a></p><p><a href=https://www.reddit.com/r/reinforcementlearning/comments/7kwcb5/need_help_how_to_debug_deep_rl_algorithms/>https://www.reddit.com/r/reinforcementlearning/comments/7kwcb5/need_help_how_to_debug_deep_rl_algorithms/</a></p><h3 id=a3c>A3C<a hidden class=anchor aria-hidden=true href=#a3c>#</a></h3><ul><li><input checked disabled type=checkbox> David Sliver</li><li><input checked disabled type=checkbox> <a href=https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63>Policy Gradient Mathe</a></li><li><input checked disabled type=checkbox> Paper review</li><li><input disabled type=checkbox> <del>Example threading Pytorch</del></li><li><input disabled type=checkbox> <del>Code for Pendulum</del></li></ul><h3 id=a2c>A2C<a hidden class=anchor aria-hidden=true href=#a2c>#</a></h3><p><a href=https://github.com/ikostrikov/pytorch-a3c>Why A2C not A3C</a></p><ul><li><a href=https://www.datahubbs.com/two-headed-a2c-network-in-pytorch/>Two head Network</a></li><li><strong>Why Entropy</strong>: <a href="https://awjuliani.medium.com/maximum-entropy-policies-in-reinforcement-learning-everyday-life-f5a1cc18d32d#:~:text=Because%20RL%20is%20all%20about,the%20actions%20an%20agent%20takes.&text=In%20RL%2C%20the%20goal%20is,term%20sum%20of%20discounted%20rewards">https://awjuliani.medium.com/maximum-entropy-policies-in-reinforcement-learning-everyday-life-f5a1cc18d32d#:~:text=Because%20RL%20is%20all%20about,the%20actions%20an%20agent%20takes.&text=In%20RL%2C%20the%20goal%20is,term%20sum%20of%20discounted%20rewards</a><ul><li>Entropy is great, but you might be wondering what that has to do with reinforcement learning and this A2C algorithm we discussed. The idea here is to use entropy to encourage further exploration of the model</li><li>to prevent premature convergence</li></ul></li><li>Negative Loss:<ul><li>TensorFlow and PyTorch currently don’t have the ability to maximize a function, we then minimize the negative of our loss</li></ul></li><li><a href=https://discuss.pytorch.org/t/how-to-implement-accumulated-gradient/3822>Accumulated gradient</a></li></ul><h3 id=ppo>PPO<a hidden class=anchor aria-hidden=true href=#ppo>#</a></h3><ul><li><p>debug for sub optimal:</p><ul><li><p><a href=https://www.reddit.com/r/reinforcementlearning/comments/d3wym2/catastrophic_unlearning_in_ppo_a_plausible_cause/>possible solutions</a></p><ul><li>decrease lr</li><li>decrease lambda during program</li></ul></li><li><p>It would be helpful to output more metrics, such as losses, norms of the gradients, KL divergence between your old and new policies after a number of PPO updates.<a href="https://www.reddit.com/r/reinforcementlearning/comments/bqh01v/having_trouble_with_ppo_rewards_crashing/?utm_source=share&utm_medium=web2x">source</a></p></li><li><p>change algo</p><blockquote><p>It depends on the environment. Something like ball balancing might just tend to destabilize with PPO, vs. something like half cheetah that is less finicky balance wise. You might try using td3 or sac, but with ppo you might just have to early stop. There might be some perfect combo of lr and clip param that leaves it stabilized&mldr; maybe with using another optimizer as well like classic momentum or adagrad.</p><p><a href="https://www.reddit.com/r/reinforcementlearning/comments/bqh01v/having_trouble_with_ppo_rewards_crashing/?utm_source=share&utm_medium=web2x">source</a></p></blockquote></li><li></li></ul></li></ul><h2 id=material>Material<a hidden class=anchor aria-hidden=true href=#material>#</a></h2><h4 id=powerup-knowledge>Powerup Knowledge<a hidden class=anchor aria-hidden=true href=#powerup-knowledge>#</a></h4><ul><li><a href=https://towardsdatascience.com/a-short-introduction-to-go-explore-c61c2ef201f0>How to Exploration</a></li><li><a href=https://www.alexirpan.com/2018/02/14/rl-hard.html>Why RL Hard</a></li><li><a href=https://www.reddit.com/r/MachineLearning/comments/bdgxin/d_any_papers_that_criticize_deep_reinforcement/>DRL Sucks</a></li></ul><h3 id=intro>Intro<a hidden class=anchor aria-hidden=true href=#intro>#</a></h3><p><a href=https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c>An Introduction to Deep Reinforcement Learning</a></p><h3 id=drl>DRL<a hidden class=anchor aria-hidden=true href=#drl>#</a></h3><h4 id=reward-shaping>Reward shaping<a hidden class=anchor aria-hidden=true href=#reward-shaping>#</a></h4><p><a href="https://www.youtube.com/watch?v=0R3PnJEisqk&ab_channel=Bonsai">reference</a></p><h4 id=code-for-model>Code for model<a hidden class=anchor aria-hidden=true href=#code-for-model>#</a></h4><ul><li><a href=https://github.com/marload/DeepRL-TensorFlow2>Tensorflow</a></li><li><a href=https://github.com/bentrevett/pytorch-rl>Pytorch</a><ul><li><a href=https://github.com/FrancescoSaverioZuppichini/Pytorch-how-and-when-to-use-Module-Sequential-ModuleList-and-ModuleDict>Simple to create Model</a></li><li><a href=https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail>PPO</a></li></ul></li></ul><h4 id=course>Course<a hidden class=anchor aria-hidden=true href=#course>#</a></h4><ul><li><p><a href=http://rail.eecs.berkeley.edu/deeprlcourse/>CS 285</a></p></li><li><p><a href=http://videolectures.net/rldm2015_silver_reinforcement_learning/>Deep Reinforcement Learning</a></p></li></ul><h4 id=blog>Blog<a hidden class=anchor aria-hidden=true href=#blog>#</a></h4><ul><li><a href=https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/>Atari</a></li><li><a href=https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0>Medium</a></li><li><a href=https://github.com/marload/DeepRL-TensorFlow2>Tensorflow Code</a></li><li><a href=https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756>Start from PONG</a></li><li><a href=https://danieltakeshi.github.io>Nice Dude</a></li></ul><h4 id=atari>Atari<a hidden class=anchor aria-hidden=true href=#atari>#</a></h4><ul><li><a href=https://hub.packtpub.com/openai-gym-environments-wrappers-and-monitors-tutorial/>Monitor / Video using X11</a></li><li><a href=https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html>Recap</a></li></ul><h4 id=cnn-1>CNN<a hidden class=anchor aria-hidden=true href=#cnn-1>#</a></h4><ul><li>parameter calculation : <a href="https://medium.com/@iamvarman/how-to-calculate-the-number-of-parameters-in-the-cnn-5bd55364d7ca#:~:text=To%20calculate%20it%2C%20we%20have,3%E2%80%931))%20%3D%2048">https://medium.com/@iamvarman/how-to-calculate-the-number-of-parameters-in-the-cnn-5bd55364d7ca#:~:text=To%20calculate%20it%2C%20we%20have,3%E2%80%931))%20%3D%2048</a></li><li><a href=https://cs231n.github.io/convolutional-networks/#pool>output shape calculation</a></li></ul><h3 id=api-for-vrep>API for Vrep<a hidden class=anchor aria-hidden=true href=#api-for-vrep>#</a></h3><p><a href=https://www.coppeliarobotics.com/helpFiles/en/legacyRemoteApiOverview.htm>Legacy remote API</a></p><p><a href=https://www.coppeliarobotics.com/helpFiles/en/remoteApiFunctionsPython.htm>Remote API functions (Python)</a></p><h3 id=david-silver---410>David Silver - 4/10<a hidden class=anchor aria-hidden=true href=#david-silver---410>#</a></h3><p><a href=https://www.davidsilver.uk/teaching/>Teaching - David Silver</a></p><h3 id=the-book---ch5>The Book - Ch5<a hidden class=anchor aria-hidden=true href=#the-book---ch5>#</a></h3><p><a href=http://incompleteideas.net/book/the-book.html>Reinforcement Learning: An Introduction</a></p><p><a href=http://incompleteideas.net/book/code/code2nd.html>Code for it:</a></p><p><a href=https://waxworksmath.com/Authors/N_Z/Sutton/RLAI_1st_Edition/sutton.html>Reinforcement Learning: An Introduction</a></p><h3 id=paper>Paper<a hidden class=anchor aria-hidden=true href=#paper>#</a></h3><p><a href=https://spinningup.openai.com/en/latest/spinningup/keypapers.html>Keypaper</a></p><h3 id=training-software>Training Software<a hidden class=anchor aria-hidden=true href=#training-software>#</a></h3><p><a href=https://gym.openai.com/>Gym: A toolkit for developing and comparing reinforcement learning algorithms</a></p><h3 id=open-cv>Open cv<a hidden class=anchor aria-hidden=true href=#open-cv>#</a></h3><ul><li><a href=https://chadrick-kwag.net/cv2-resize-interpolation-methods/>resize</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://Vacationwkx.github.io/tags/reinforcement-learning/>reinforcement learning</a></li></ul><nav class=paginav><a class=prev href=https://Vacationwkx.github.io/posts/ai/><span class=title>« Prev Page</span><br><span>AI</span></a>
<a class=next href=https://Vacationwkx.github.io/posts/nn-notes/><span class=title>Next Page »</span><br><span>NNN notes</span></a></nav></footer><div id=disqus_thread></div><script>(function(){var a=document,b=a.createElement('script');b.src='https://github-on-hugo.disqus.com/embed.js',b.setAttribute('data-timestamp',+new Date),(a.head||a.body).appendChild(b)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></article></main><footer class=footer><span>&copy; 2021 <a href=https://Vacationwkx.github.io>無極</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button>
</a><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script><script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script></body></html>