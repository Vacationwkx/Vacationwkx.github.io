<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>epsilon-Greedy vs epsilon-Soft | 無極</title><meta name=keywords content="reinforcement learning"><meta name=description content="In reinforcement learning, we can&rsquo;t run infinite times to update the whole $Q$ - value table or $V$ - value table, efficient update choices must be made.
Generally thinking, which $s$ or $(s,a)$ has more opportunity to get $R$ (high value), should be updated more to converge to the optimal. But stochastic exploring is also required to jump out of sub-optimal. The simple idea to implement is $\epsilon$ -greedy.
Tips:"><meta name=author content="Vac"><link rel=canonical href=https://Vacationwkx.github.io/posts/rl-epsilon-policy/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.04512c372388e08b5118f5b117b2d3efef4ddae52017e16085c8d8d4e361c43d.css integrity="sha256-BFEsNyOI4ItRGPWxF7LT7+9N2uUgF+FghcjY1ONhxD0=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://Vacationwkx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://Vacationwkx.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Vacationwkx.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Vacationwkx.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://Vacationwkx.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.83.0"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="epsilon-Greedy vs epsilon-Soft"><meta property="og:description" content="In reinforcement learning, we can&rsquo;t run infinite times to update the whole $Q$ - value table or $V$ - value table, efficient update choices must be made.
Generally thinking, which $s$ or $(s,a)$ has more opportunity to get $R$ (high value), should be updated more to converge to the optimal. But stochastic exploring is also required to jump out of sub-optimal. The simple idea to implement is $\epsilon$ -greedy.
Tips:"><meta property="og:type" content="article"><meta property="og:url" content="https://Vacationwkx.github.io/posts/rl-epsilon-policy/"><meta property="og:image" content="https://Vacationwkx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-11-30T11:35:49+08:00"><meta property="article:modified_time" content="2020-11-30T11:35:49+08:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Vacationwkx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="epsilon-Greedy vs epsilon-Soft"><meta name=twitter:description content="In reinforcement learning, we can&rsquo;t run infinite times to update the whole $Q$ - value table or $V$ - value table, efficient update choices must be made.
Generally thinking, which $s$ or $(s,a)$ has more opportunity to get $R$ (high value), should be updated more to converge to the optimal. But stochastic exploring is also required to jump out of sub-optimal. The simple idea to implement is $\epsilon$ -greedy.
Tips:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://Vacationwkx.github.io/posts/"},{"@type":"ListItem","position":3,"name":"epsilon-Greedy vs epsilon-Soft","item":"https://Vacationwkx.github.io/posts/rl-epsilon-policy/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"epsilon-Greedy vs epsilon-Soft","name":"epsilon-Greedy vs epsilon-Soft","description":"In reinforcement learning, we can\u0026rsquo;t run infinite times to update the whole $Q$ - value table or $V$ - value table, efficient update choices must be made.\nGenerally thinking, which $s$ or $(s,a)$ has more opportunity to get $R$ (high value), should be updated more to converge to the optimal. But stochastic exploring is also required to jump out of sub-optimal. The simple idea to implement is $\\epsilon$ -greedy.\nTips:","keywords":["reinforcement learning"],"articleBody":"In reinforcement learning, we can’t run infinite times to update the whole $Q$ - value table or $V$ - value table, efficient update choices must be made.\nGenerally thinking, which $s$ or $(s,a)$ has more opportunity to get $R$ (high value), should be updated more to converge to the optimal. But stochastic exploring is also required to jump out of sub-optimal. The simple idea to implement is $\\epsilon$ -greedy.\nTips:\nTo generate a list of probability of action set, which sum to 1, we can use Dirichlet distribution, e.g.\np=numpy.random.dirichlet(np.ones(n_actions)) $\\epsilon$ -Greedy $$ \\begin{aligned}\\max \u0026: p=1-\\epsilon+\\frac{\\epsilon}{|A|}\\\\ \\mathrm{others}\u0026: p=\\frac{\\epsilon}{|A|}\\end{aligned} $$\ni.e, random chose with $\\epsilon$ and chose the best with $1-\\epsilon$\nwith code\np[max_value_index]=1-epsilon p+=epsilon/n $\\epsilon$ - Soft the only requirement of $\\epsilon$ - Soft is each probability greater than $\\epsilon /|A|$, Dirichlet is useful here.\np=np.random.dirichlet(np.ones(n))*(1-epsilon) p+=epsilon/n ","wordCount":"137","inLanguage":"en","datePublished":"2020-11-30T11:35:49+08:00","dateModified":"2020-11-30T11:35:49+08:00","author":{"@type":"Person","name":"Vac"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Vacationwkx.github.io/posts/rl-epsilon-policy/"},"publisher":{"@type":"Organization","name":"無極","logo":{"@type":"ImageObject","url":"https://Vacationwkx.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://Vacationwkx.github.io accesskey=h title="Home (Alt + H)"><img src=/apple-touch-icon.png alt=logo aria-label=logo height=35>Home</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://Vacationwkx.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://Vacationwkx.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://Vacationwkx.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://Vacationwkx.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Vacationwkx.github.io>Home</a>&nbsp;»&nbsp;<a href=https://Vacationwkx.github.io/posts/>Posts</a></div><h1 class=post-title>epsilon-Greedy vs epsilon-Soft</h1><div class=post-meta>November 30, 2020&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Vac</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#epsilon--greedy aria-label="$\epsilon$ -Greedy">$\epsilon$ -Greedy</a></li><li><a href=#epsilon---soft aria-label="$\epsilon$ - Soft">$\epsilon$ - Soft</a></li></ul></div></details></div><div class=post-content><p>In reinforcement learning, we can&rsquo;t run infinite times to update the whole $Q$ - value table or $V$ - value table, efficient update choices must be made.</p><p>Generally thinking, which $s$ or $(s,a)$ has more opportunity to get $R$ (high value), should be updated more to converge to the optimal. But stochastic exploring is also required to jump out of sub-optimal. The simple idea to implement is $\epsilon$ -greedy.</p><p><strong>Tips:</strong></p><p>To generate a list of probability of action set, which sum to 1, we can use Dirichlet distribution, e.g.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>p<span style=color:#f92672>=</span>numpy<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>dirichlet(np<span style=color:#f92672>.</span>ones(n_actions))
</code></pre></div><p><img loading=lazy src=/rl/dirichlet.png alt=Dirichlet></p><h1 id=epsilon--greedy>$\epsilon$ -Greedy<a hidden class=anchor aria-hidden=true href=#epsilon--greedy>#</a></h1><p>$$
\begin{aligned}\max &: p=1-\epsilon+\frac{\epsilon}{|A|}\\ \mathrm{others}&: p=\frac{\epsilon}{|A|}\end{aligned}
$$</p><p>i.e, random chose with $\epsilon$ and chose the best with $1-\epsilon$</p><p><img loading=lazy src=/rl/e_greedy.png alt=epsilon-greedy></p><p>with code</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>p[max_value_index]<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>epsilon
p<span style=color:#f92672>+=</span>epsilon<span style=color:#f92672>/</span>n
</code></pre></div><h1 id=epsilon---soft>$\epsilon$ - Soft<a hidden class=anchor aria-hidden=true href=#epsilon---soft>#</a></h1><p>the only requirement of $\epsilon$ - Soft is each probability greater than $\epsilon /|A|$, Dirichlet is useful here.</p><p><img loading=lazy src=/rl/e_soft.png alt=epsilon-greedy></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>p<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>dirichlet(np<span style=color:#f92672>.</span>ones(n))<span style=color:#f92672>*</span>(<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>epsilon)
p<span style=color:#f92672>+=</span>epsilon<span style=color:#f92672>/</span>n
</code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://Vacationwkx.github.io/tags/reinforcement-learning/>reinforcement learning</a></li></ul><nav class=paginav><a class=prev href=https://Vacationwkx.github.io/posts/ssh-jupyter/><span class=title>« Prev Page</span><br><span>Use Jupiter Notebook by ssh</span></a>
<a class=next href=https://Vacationwkx.github.io/posts/rl-update-equation/><span class=title>Next Page »</span><br><span>Value Update Comparsion among Basical RL</span></a></nav></footer><div id=disqus_thread></div><script>(function(){var a=document,b=a.createElement('script');b.src='https://github-on-hugo.disqus.com/embed.js',b.setAttribute('data-timestamp',+new Date),(a.head||a.body).appendChild(b)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></article></main><footer class=footer><span>&copy; 2021 <a href=https://Vacationwkx.github.io>無極</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button>
</a><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script><script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script></body></html>